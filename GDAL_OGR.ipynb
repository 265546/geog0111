{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 GDAL and OGR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#3.3-GDAL-and-OGR\" data-toc-modified-id=\"3.3-GDAL-and-OGR-3.1\">3.3 GDAL and OGR</a></span><ul class=\"toc-item\"><li><span><a href=\"#3.3.1-The-MODIS-data\" data-toc-modified-id=\"3.3.1-The-MODIS-data-3.1.1\">3.3.1 The MODIS data</a></span><ul class=\"toc-item\"><li><span><a href=\"#3.3.1.1-File-Naming-Convention\" data-toc-modified-id=\"3.3.1.1-File-Naming-Convention-3.1.1.1\">3.3.1.1 File Naming Convention</a></span></li><li><span><a href=\"#3.3.1.2-Dataset-Naming-Convention\" data-toc-modified-id=\"3.3.1.2-Dataset-Naming-Convention-3.1.1.2\">3.3.1.2 Dataset Naming Convention</a></span></li></ul></li><li><span><a href=\"#3.3.2-MODIS-dataset-access\" data-toc-modified-id=\"3.3.2-MODIS-dataset-access-3.1.2\">3.3.2 MODIS dataset access</a></span><ul class=\"toc-item\"><li><span><a href=\"#3.3.2.1-gdal.ReadAsArray()\" data-toc-modified-id=\"3.3.2.1-gdal.ReadAsArray()-3.1.2.1\">3.3.2.1 <code>gdal.ReadAsArray()</code></a></span></li><li><span><a href=\"#3.3.2.2-Metadata\" data-toc-modified-id=\"3.3.2.2-Metadata-3.1.2.2\">3.3.2.2 Metadata</a></span></li></ul></li><li><span><a href=\"#3.3.3-Reading-and-displaying-data\" data-toc-modified-id=\"3.3.3-Reading-and-displaying-data-3.1.3\">3.3.3 Reading and displaying data</a></span><ul class=\"toc-item\"><li><span><a href=\"#3.3.3.1-glob\" data-toc-modified-id=\"3.3.3.1-glob-3.1.3.1\">3.3.3.1 <code>glob</code></a></span></li><li><span><a href=\"#3.3.3.2-reading-and-displaying-image-data\" data-toc-modified-id=\"3.3.3.2-reading-and-displaying-image-data-3.1.3.2\">3.3.3.2 reading and displaying image data</a></span></li><li><span><a href=\"#3.3.3.3-subplot-plotting\" data-toc-modified-id=\"3.3.3.3-subplot-plotting-3.1.3.3\">3.3.3.3 subplot plotting</a></span></li><li><span><a href=\"#3.3.3.3-tile-stitching\" data-toc-modified-id=\"3.3.3.3-tile-stitching-3.1.3.4\">3.3.3.3 tile stitching</a></span></li><li><span><a href=\"#3.3.3.4-gdal-virtual-file\" data-toc-modified-id=\"3.3.3.4-gdal-virtual-file-3.1.3.5\">3.3.3.4 <code>gdal</code> virtual file</a></span></li></ul></li><li><span><a href=\"#The-country-borders-dataset\" data-toc-modified-id=\"The-country-borders-dataset-3.1.4\">The country borders dataset</a></span></li><li><span><a href=\"#Putting-it-all-together\" data-toc-modified-id=\"Putting-it-all-together-3.1.5\">Putting it all together</a></span></li><li><span><a href=\"#Exercises\" data-toc-modified-id=\"Exercises-3.1.6\">Exercises</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "In this section, we'll look at combining both raster and vector data to provide a masked dataset ready to use. We will produce a combined dataset of leaf area index (LAI) over the UK derived from the MODIS sensor. The MODIS LAI product is produced every 4 days and it is provided spatially tiled. Each tile covers around 1200 km x 1200 km of the Earth's surface. Below you can see a map showing the MODIS tiling convention.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.1 The MODIS LAI data\n",
    "\n",
    "You should by now be able to download MODIS data, but in this case, the data are provided (or downloaded for you) in the `data` folder as files `MCD15A3H.A2018273.h17v03.006.2018278143630.hdf`  and `MCD15A3H.A2018273.h18v03.006.2018278143633.hdf` by running the code below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geog0111.geog_data import *\n",
    "\n",
    "filenames = ['MCD15A3H.A2018273.h17v03.006.2018278143630.hdf', \\\n",
    "            'MCD15A3H.A2018273.h18v03.006.2018278143633.hdf']\n",
    "destination_folder=\"data\"\n",
    "\n",
    "for file_name in filenames:\n",
    "    f = procure_dataset(file_name,verbose=True,\\\n",
    "                        destination_folder=destination_folder)\n",
    "    print(file_name,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to select the LAI layers, so let's have a look at the contents ('sub datasets') of one of the files.\n",
    "\n",
    "To do this with `gdal`:\n",
    "\n",
    "* make the full filename (folder name, plus the filename in that folder). Use `Path` for this, but convert to a string.\n",
    "* open the file, store as `g`\n",
    "* get the list `g.GetSubDatasets()` and loop over this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdal\n",
    "from pathlib import Path\n",
    "from geog0111.geog_data import *\n",
    "\n",
    "filenames = ['MCD15A3H.A2018273.h17v03.006.2018278143630.hdf', \\\n",
    "            'MCD15A3H.A2018273.h18v03.006.2018278143633.hdf']\n",
    "destination_folder=\"data\"\n",
    "\n",
    "for file_name in filenames:\n",
    "    # form full filename as a string\n",
    "    # and print with an underline of = \n",
    "    file_name = Path(destination_folder).joinpath(file_name).as_posix()\n",
    "    print(file_name)\n",
    "    print('='*len(file_name))\n",
    "    \n",
    "    # open the file as g\n",
    "    g = gdal.Open(file_name)\n",
    "    # loop over the subdatasets\n",
    "    for d in g.GetSubDatasets():\n",
    "        print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that the data is in `HDF4` format, and that it has a number of layers. The dataset/layer we're interested in\n",
    "\n",
    "`HDF4_EOS:EOS_GRID:\"data/MCD15A3H.A2018273.h18v03.006.2018278143633.hdf\":MOD_Grid_MCD15A3H:Lai_500m`.\n",
    "\n",
    "\n",
    "### 3.3.1.1 File Naming Convention\n",
    "\n",
    "This section taken from [NASA MODIS product page](https://nsidc.org/data/mod10a1).\n",
    "\n",
    "Example File Name:\n",
    "\n",
    "`data/MOD10A1.A2000055.h15v01.006.2016061160800.hdf`\n",
    "\n",
    "\n",
    "\n",
    "`FOLDER/MOD[PID].A[YYYY][DDD].h[NN]v[NN].[VVV].[yyyy][ddd][hhmmss].hdf`\n",
    "\n",
    "Refer to Table 3.3.1 for descriptions of the file name variables listed above.\n",
    "\n",
    "\n",
    "\n",
    "|  Variable | Description  |  \n",
    "|---|---|\n",
    "| FOLDER| folder/directory name of file|\n",
    "| MOD  |  MODIS/Terra  (`MCD` means combined)| \n",
    "|  PID |   Product ID|  \n",
    "| A\t|Acquisition date follows|\n",
    "|YYYY\t|Acquisition year|\n",
    "|DDD\t|Acquisition day of year|\n",
    "|h[NN]v[NN]\t|Horizontal tile number and vertical tile number (see Grid for details.)|\n",
    "|VVV\t|Version (Collection) number|\n",
    "|yyyy\t|Production year|\n",
    "|ddd\t|Production day of year|\n",
    "|hhmmss\t|Production hour/minute/second in GMT|\n",
    "|.hdf\t|HDF-EOS formatted data file|\n",
    "Table 3.3.1. Variables in the MODIS File Naming Convention\n",
    "\n",
    "![](https://nsidc.org/sites/nsidc.org/files/images/modis-sin-grid.png)\n",
    "\n",
    "\n",
    "### 3.3.1.2 Dataset Naming Convention\n",
    "\n",
    "Example Dataset Name:\n",
    "\n",
    "`HDF4_EOS:EOS_GRID:\"data/MCD15A3H.A2018273.h18v03.006.2018278143633.hdf\":MOD_Grid_MCD15A3H:Lai_500m`\n",
    "\n",
    "\n",
    "`FORMAT:\"FILENAME\":MOD_Grid_PRODUCT:LAYER`\n",
    "\n",
    "|  Variable | Description  |  \n",
    "|---|---|\n",
    "|FORMAT| file format, `HDF4_EOS:EOS_GRID`|\n",
    "|FILENAME| dataset file name, see below|\n",
    "|PRODUCT| MODIS product code e.g. `MCD15A3H`|\n",
    "|LAYER| sub-dataset name e.g. `Lai_500m`|\n",
    "Table 3.3.2. Variables in the MODIS Dataset Naming Convention\n",
    "\n",
    "\n",
    "**Exercise E3.3.1**\n",
    "\n",
    "* Check you're happy that the other datasets (e.g. `LaiStdDev_500m`) follow the same convention as `Lai_500m`\n",
    "* work out what the dataset/layer name would be for the dataset product `MOD10A1` version `6` for the $1^{st}$ January 2018, for tile `h25v06` for the layer `NDSI_Snow_Cover`. You will find product information [on the relevant NASA page](https://nsidc.org/data/mod10a1). You may not be able to access the production date/time, but just put a placeholder for that now.\n",
    "* phrase the filename and layer name as '`f`' strings, e.g. starting `f'HDF4_EOS:EOS_GRID:\"{filename}\":MOD_Grid_{}'` etc.\n",
    "\n",
    "**Hint**:\n",
    "\n",
    "You can explore the filenames by looking into the [Earthdata link](https://n5eil01u.ecs.nsidc.org/MOSA/).\n",
    "\n",
    "![](https://n5eil01u.ecs.nsidc.org/MOSA/MYD10A1.006/2018.01.01/BROWSE.MYD10A1.A2018001.h25v05.006.2018003025825.1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do exercise here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.2 MODIS dataset access\n",
    "\n",
    "### 3.3.2.1 `gdal.ReadAsArray()`\n",
    "\n",
    "We can now access the dataset names and open the datasets in `gdal` directly, e.g.:\n",
    "\n",
    "`HDF4_EOS:EOS_GRID:\"data/MCD15A3H.A2018273.h18v03.006.2018278143633.hdf\":MOD_Grid_MCD15A3H:Lai_500m`\n",
    "\n",
    "We can read the dataset with `g.ReadAsArray()`, after we have opened it. It returns a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdal\n",
    "import numpy as np\n",
    "\n",
    "filename = 'data/MCD15A3H.A2018273.h17v03.006.2018278143630.hdf'\n",
    "dataset_name = f'HDF4_EOS:EOS_GRID:\"{filename:s}\":MOD_Grid_MCD15A3H:Lai_500m'\n",
    "print(f\"dataset: {dataset_name}\")\n",
    "\n",
    "g = gdal.Open(dataset_name)\n",
    "data = g.ReadAsArray()\n",
    "\n",
    "print(type(data))\n",
    "print('max:',data.max())\n",
    "print('max:',data.min())\n",
    "# get unique values, for interst\n",
    "print('unique values:',np.unique(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise E3.3.2**\n",
    "\n",
    "* print out some further summary statistics of the dataset\n",
    "* print out the data type and `shape`\n",
    "* how many rows and columns does the dataset have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do exercise here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2.2 Metadata\n",
    "\n",
    "There will generally be a set of metadata associated with a geospatial dataset. This will describe e.g. the processing chain, special codes in the dataset, and projection and other information.\n",
    "\n",
    "In `gdal`, w access the metedata using `g.GetMetadata()`. A dictionary is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/MCD15A3H.A2018273.h17v03.006.2018278143630.hdf'\n",
    "dataset_name = f'HDF4_EOS:EOS_GRID:\"{filename:s}\":MOD_Grid_MCD15A3H:Lai_500m'\n",
    "g = gdal.Open(dataset_name)\n",
    "\n",
    "print (\"\\nMetedata Keys:\\n\")\n",
    "# get the metadata dictionary keys\n",
    "for k in g.GetMetadata().keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some of these metadata fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdal\n",
    "import numpy as np\n",
    "\n",
    "filename = 'data/MCD15A3H.A2018273.h17v03.006.2018278143630.hdf'\n",
    "dataset_name = f'HDF4_EOS:EOS_GRID:\"{filename:s}\":MOD_Grid_MCD15A3H:Lai_500m'\n",
    "print(f\"dataset: {dataset_name}\")\n",
    "\n",
    "g = gdal.Open(dataset_name)\n",
    "# get the metadata dictionary keys\n",
    "for k in [\"LONGNAME\",\"CHARACTERISTICBINSIZE500M\",\\\n",
    "          \"MOD15A2_FILLVALUE_DOC\",\\\n",
    "          \"GRINGPOINTLATITUDE.1\",\"GRINGPOINTLONGITUDE.1\",\\\n",
    "          'scale_factor']:\n",
    "    print(k,g.GetMetadata()[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that the datasets use the MODIS Sinusoidal projection. Also we see that the pixel spacing is around 463m, there is a scale factor of 0.1 to be applied etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise E3.3.3**\n",
    "\n",
    "look at the metadata to discover:\n",
    "\n",
    "* the number of rows and columns in the dataset\n",
    "* the range of valid values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do exercise here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.3 Reading and displaying data\n",
    "\n",
    "### 3.3.3.1 `glob`\n",
    "\n",
    "Let us now suppose that we want to examine an `hdf` file that we have previously downloaded and stored in the directiory `data`.\n",
    "\n",
    "How can we get a view into this directory to the the names of the files there?\n",
    "\n",
    "The answer to this is `glob`, which we can access from the `pathlib` module.\n",
    "\n",
    "Let's look in the `data` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# look in this directory\n",
    "in_directory = Path('data')\n",
    "\n",
    "filenames = in_directory.glob('*')\n",
    "print('files in the directory',in_directory,':')\n",
    "for f in filenames:\n",
    "    print(f.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the argument `'data/*'` where `*` is a wildcard. Any filenames that match this pattern will be returned as a list.\n",
    "\n",
    "If we want the list sorted, we need to use the `sorted()` method. This is similar to the list `sort` we have seen previously, but returns the sorted list.\n",
    "\n",
    "The wildcard `*` here means a match to zero or more characters, so this is matching all names in the directory `data`. The wildcard `**` would mean [all files here and all sub-directories](https://docs.python.org/3/library/pathlib.html#pathlib.Path.glob).\n",
    "\n",
    "\n",
    "\n",
    "We could be more subtle with this, e.g. matching only files ending `hdf`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "filenames = sorted(Path('data').glob('*'))\n",
    "\n",
    "for f in filenames:\n",
    "    print(f.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.3.4**\n",
    "\n",
    "* adapt the code above to return only hdf filenames for the tile `h18v03`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do exercise here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3.2 reading and displaying image data\n",
    "\n",
    "Let's now read some data as above.\n",
    "\n",
    "we do this with:\n",
    "\n",
    "    g.Open(gdal_fname)\n",
    "    data = g.ReadAsArray()\n",
    "    \n",
    "Originally the data are `uint8` (unsigned 8 bit data), but we need to multiply them by `scale_factor` (0.1 here) to convert to physical units. This also casts the data type to `float`.\n",
    "\n",
    "We can straightforwardly plot the images using `matplotlib`. We first importt the library:\n",
    "\n",
    "    import matplotlib.pylab as plt\n",
    "    \n",
    "Then set up the figure size:\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    \n",
    "Plot the image:\n",
    "\n",
    "    plt.imshow( data, vmin=0, vmax=6,cmap=plt.cm.inferno_r)\n",
    "    \n",
    "where here `data` is a 2-D dataset. We can set limits to the image scaling (`vmin`, `vmax`), so that we emphasise a particular range of values, and we can apply custom colourmaps (`cmap=plt.cm.inferno_r`).\n",
    "\n",
    "Finally here, we set a title, and plot a colour wedge to show the data scale. The `scale=0.8` here allows us to align the size of the scale with the plotted image size.\n",
    "\n",
    "    plt.title(dataset_name)\n",
    "    plt.colorbar(shrink=0.8)\n",
    "    \n",
    "If we want to save the plotted image to a file, e.g. in the directory `images`, we use:\n",
    "\n",
    "    plt.savefig(out_filename)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdal\n",
    "from pathlib import Path\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "# get only v03 hdf names\n",
    "filenames = sorted(Path('data').glob('*v03*.hdf'))\n",
    "\n",
    "\n",
    "out_directory = Path('images')\n",
    "\n",
    "for filename in filenames:\n",
    "    # pull the tile name from the filename\n",
    "    # to use as plot title\n",
    "    tile = filename.name.split('.')[2]\n",
    "    \n",
    "    dataset_name = f'HDF4_EOS:EOS_GRID:\"{str(filename):s}\\\":MOD_Grid_MCD15A3H:Lai_500m'\n",
    "    g = gdal.Open(dataset_name)\n",
    "    data = g.ReadAsArray()\n",
    "    scale_factor = float(g.GetMetadata()['scale_factor'])\n",
    "    \n",
    "    print(dataset_name,scale_factor)\n",
    "    print('*'*len(dataset_name))\n",
    "    print(type(data),data.dtype,data.shape,'\\n')\n",
    "    \n",
    "    data = data * scale_factor\n",
    "    print(type(data),data.dtype,data.shape,'\\n')\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow( data, vmin=0, vmax=6,cmap=plt.cm.inferno_r)\n",
    "    plt.title(tile)\n",
    "    plt.colorbar(shrink=0.8)\n",
    "    \n",
    "    # save figure as png\n",
    "    plot_name = filename.stem + '.png'\n",
    "    print(plot_name)\n",
    "    out_filename = out_directory.joinpath(plot_name)\n",
    "    plt.savefig(out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the images we saved are there!\n",
    "# and access some file info while we are here\n",
    "# using pathlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "for f in Path('images').glob('MCD*v03*.png'):\n",
    "    \n",
    "    # get the file size in bytes \n",
    "    size_in_B = f.stat().st_size\n",
    "    \n",
    "    # get the file modification time (ns)\n",
    "    mod_date_ns = f.stat().st_mtime_ns\n",
    "    mod_date = datetime.fromtimestamp(mod_date_ns // 1000000000)\n",
    "    \n",
    "    print(f'{f} {size_in_B} Bytes {mod_date}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3.3 subplot plotting\n",
    "\n",
    "Often, we want to have several figures on the same plot. We can do this with `plt.subplots()`:\n",
    "\n",
    "The way we set the title and other features is slightly diifferent, but there are many example of different plot types on the web we can follow as examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdal\n",
    "from pathlib import Path\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "filenames = sorted(Path('data').glob('*v03*.hdf'))\n",
    "\n",
    "out_directory = Path('images')\n",
    "\n",
    "'''\n",
    "Set up subplots of 1 row x 2 columns\n",
    "'''\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, sharex=True, sharey=True,\n",
    "                       figsize=(10,5))\n",
    "# need to force axs collapse to a 2D array\n",
    "# for indexing to be easy T here is transpose\n",
    "# to get row/col the right way around\n",
    "axs = np.array(axs).T.flatten()\n",
    "\n",
    "for i,filename in enumerate(filenames):\n",
    "    # pull the tile name from the filename\n",
    "    # to use as plot title\n",
    "    tile = filename.name.split('.')[2]\n",
    "    \n",
    "    \n",
    "    dataset_name = f'HDF4_EOS:EOS_GRID:\"{str(filename):s}\\\":MOD_Grid_MCD15A3H:Lai_500m'\n",
    "    g = gdal.Open(dataset_name)\n",
    "    data = g.ReadAsArray() * float(g.GetMetadata()['scale_factor'])\n",
    "\n",
    "    img = axs[i].imshow(data, interpolation=\"nearest\", vmin=0, vmax=4,\n",
    "                 cmap=plt.cm.inferno_r)\n",
    "    axs[i].set_title(tile)\n",
    "    plt.colorbar(img,ax=axs[i],shrink=0.7)\n",
    "    \n",
    "# save figure as pdf this time\n",
    "plot_name = 'joinedup.pdf'\n",
    "print(plot_name)\n",
    "out_filename = out_directory.joinpath(plot_name)\n",
    "plt.savefig(out_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.3.5**\n",
    "\n",
    "* Use the code in [3.3.1](#3.3.1-The-MODIS-data) to get the datasets for the tiles `h17v04` and `h18v04`:\n",
    "\n",
    "    MCD15A3H.A2018273.h17v04.006.2018278143630.hdf\t\n",
    "    MCD15A3H.A2018273.h18v04.006.2018278143638.hdf\n",
    "\n",
    "* use subplot as above to plot a 2x2 set of subplots of these data.\n",
    "\n",
    "\n",
    "**Hint**\n",
    "\n",
    "The code should look much like that above, but you need to give the fiuller list of filenames and set the subplot shape.\n",
    "\n",
    "The result should look like:\n",
    "\n",
    "![](images/joinedup4.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do exercise here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3.3 tile stitching\n",
    "\n",
    "You may want to generate a single view of the 4 tiles.\n",
    "\n",
    "We could achieve this by stitching things together \"by hand\"...\n",
    "\n",
    "**recipe:**\n",
    "\n",
    "* First, lets generate a 3D dataset with all 4 tiles, so we have the images stored as members of a list `data[0]`,`data[1]`,`data[2]` and `data[3]`:\n",
    "\n",
    "        data = []\n",
    "        for filename in filenames:\n",
    "            dataname = f'HDF4_EOS:EOS_GRID:\"{str(filename):s}\":MOD_Grid_MCD15A3H:Lai_500m'\n",
    "            g = gdal.Open(dataname)\n",
    "            data.append(g.ReadAsArray() * scale)\n",
    "\n",
    "* then, we produce vertical stacks of the first two and last two files. This can be done in various ways, but it is perhaps clearest to use `np.vstack()`\n",
    "\n",
    "        top = np.vstack([data[0],data[1]])\n",
    "        bot = np.vstack([data[2],data[3]])\n",
    "        \n",
    "* then, produce a horizontal stack of these stacks:\n",
    "\n",
    "        lai_stich = np.hstack([top,bot])\n",
    "        \n",
    "and plot the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdal\n",
    "from pathlib import Path\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "scale = 0.1\n",
    "\n",
    "filenames = sorted(Path('data').glob('*v0*.hdf'))\n",
    "\n",
    "data = []\n",
    "for filename in filenames:\n",
    "    dataname = f'HDF4_EOS:EOS_GRID:\"{str(filename)}\":MOD_Grid_MCD15A3H:Lai_500m'\n",
    "    g = gdal.Open(dataname)\n",
    "    # append each image to the data list\n",
    "    data.append(g.ReadAsArray() * scale)\n",
    "\n",
    "top = np.vstack([data[0],data[1]])\n",
    "bot = np.vstack([data[2],data[3]])\n",
    "\n",
    "lai_stich = np.hstack([top,bot])\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(lai_stich, interpolation=\"nearest\", vmin=0, vmax=4,\n",
    "          cmap=plt.cm.inferno_r)\n",
    "plt.colorbar(shrink=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.3.6**\n",
    "\n",
    "* examine how the `vstack` and `hstack` methods work. Print out the shape of the array after stacking to appreciate this.\n",
    "* how big (in pixels) is the whole dataset now? \n",
    "* If a `float` is 64 bits, how many bytes is this data array likely to be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do exercise here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3.4 `gdal` virtual file\n",
    "\n",
    "However, stitching in this way is problematic if you want to mosaic many tiles, as you need to read in all the data in memory. Also,some tiles may be missing. GDAL allows you to create a mosaic as [virtual file format](https://www.gdal.org/gdal_vrttut.html), using gdal.BuildVRT (check the documentation). \n",
    "\n",
    "This function takes two inputs: the output filename (`stitch_up.vrt`) and a set of GDAL format filenames. It returns the open output dataset, so that we can check what it looks like with e.g. `gdal.Info`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdal\n",
    "from pathlib import Path\n",
    "\n",
    "# need to convert filenames to strings\n",
    "# which we can do with p.as_posix() or str(p)\n",
    "filenames = sorted([p.as_posix() for p in Path('data').glob('*v0*.hdf')])\n",
    "    \n",
    "stitch_vrt = gdal.BuildVRT(\"stitch_up.vrt\", filenames)\n",
    "\n",
    "print(gdal.Info(stitch_vrt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that we now have 4800 columns by 4800 rows dataset, centered around 0 degrees North, 0 degrees W. Let's plot the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stitch_vrt.GetSubDatasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stitch_vrt is an already opened GDAL dataset, needs to be read in\n",
    "plt.imshow(stitch_vrt.ReadAsArray()*stitch_vrt.Scale,\n",
    "           interpolation=\"nearest\", vmin=0, vmax=4, \n",
    "          cmap=plt.cm.inferno_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The country borders dataset\n",
    "\n",
    "A number of vectors with countries and administrative subdivisions are available. The [TM_WORLD_BORDERS shapefile](http://thematicmapping.org/downloads/TM_WORLD_BORDERS-0.3.zip) is popular and in the public domain. You can see it, and have a look at the data [here](https://koordinates.com/layer/7354-tm-world-borders-03/). We need to download and unzip this file... We'll use requests as before, and we'll unpack the zip file using [`shutil.unpack_archive`](https://docs.python.org/3/library/shutil.html#shutil.unpack_archive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recall from the metadata in `valid_range` that the range of valid values in the dataset is 0 to 100. `MOD15A2_FILLVALUE_DOC` shows that numbers `249` to `254` mean that the land cover class implies no LAI. The `_Fillvalue`  is `255` here. \n",
    "\n",
    "We generally need to consider carefully what we want to do about fill values (i.e. no reading) or data for which we supect the quality is poor. This will often involve some form of iterpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = \"MOD15A2_FILLVALUE_DOC\"\n",
    "print(k,g.GetMetadata()[k])\n",
    "\n",
    "k = 'valid_range'\n",
    "print(k,g.GetMetadata()[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import shutil \n",
    "\n",
    "tm_borders_url = \"http://thematicmapping.org/downloads/TM_WORLD_BORDERS-0.3.zip\"\n",
    "\n",
    "r = requests.get(tm_borders_url)\n",
    "with open(\"data/TM_WORLD_BORDERS-0.3.zip\", 'wb') as fp:\n",
    "    fp.write (r.content)\n",
    "\n",
    "shutil.unpack_archive(\"data/TM_WORLD_BORDERS-0.3.zip\",\n",
    "                     extract_dir=\"data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have the relevant files available in your `data` folder! We can then inspect the dataset using the command line tool `ogrinfo`. We can call it from the shell by appending the `!` symbol, and select that we want to check only the data for the UK (stored in the `FIPS` field with value `UK`):\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "  It is worth noting that using OGR's queries trying to match a string, the string needs to be surrounded by <pre>'</pre>. You can also use more complicated SQL queries if you wanted to.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ogrinfo -al data/TM_WORLD_BORDERS-0.3.shp "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We inmediately see that the coordinates for the UK are in several polygons, and in WGS84 (Latitude and Longitude in decimal degrees). This is incompatible with the MODIS data.\n",
    "\n",
    "We can use GDAL to quickly apply the vector feature for the UK as a mask. There are several ways of doing this, but the simplest is to use [gdal.Warp](https://www.gdal.org/gdalwarp.html) (the link is to the command line tool). In this case, we just want to create:\n",
    "\n",
    "* an in-memory (i.e. not saved to a file) dataset. We can use the format `MEM`, so no file is written out\n",
    "* where the FIPS field is equal to \"UK\", we want the LAI to show, elsewhere, we set it to (e.g.) \"no data\" (e.g. -999)\n",
    "\n",
    "\n",
    "The mosaicked version of the MODIS LAI product is in file `stitch_up.vrt`. Since we're not saving the output to a file (`MEM` output option), we can leave the output as an empty string `\"\"`. The shapefile comes with the `cutline` options:\n",
    "\n",
    "* `cutlineDSName` that's the name of the vector file we want to use as a cutline\n",
    "* `cutlineWhere` that's the selection statement for the attribute table in the dataset. \n",
    "\n",
    "To set the no data value to 200, we can use the option `dstNodata=200`. This is because very large values in the LAI product are already indicated to be invalid.\n",
    "\n",
    "We can then just very quickly perform this and check..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = gdal.Warp(\"\", \"stitch_up.vrt\",\n",
    "         format = 'MEM',dstNodata=200,\n",
    "          cutlineDSName = 'data/TM_WORLD_BORDERS-0.3.shp', cutlineWhere = \"FIPS='UK'\")\n",
    "masked_lai = g.ReadAsArray()/10.\n",
    "plt.imshow(masked_lai, interpolation=\"nearest\", vmin=0, vmax=6, \n",
    "          cmap=plt.cm.inferno_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that works as expected, but since we haven't actually told GDAL anything about the output (other than apply the mask), we still have a 4800 pixel wide dataset. You may want to crop it by looking for where the original dataset is  *not* -999, or in fact, for pixels that are bigger than 0. You'll be pleased to know that this is a great slicing application!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lai = g.ReadAsArray()\n",
    "mask = np.nonzero(lai < 20)\n",
    "min_y = mask[0].min()\n",
    "max_y = mask[0].max() + 1\n",
    "\n",
    "min_x = mask[1].min()\n",
    "max_x = mask[1].max() + 1\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.imshow( lai[min_y:max_y,\n",
    "               min_x:max_x]/10., vmin=0, vmax=6,\n",
    "           cmap=plt.cm.inferno_r)\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "The previous steps can be put together in a function. It is even possible to simplify things a bit more by not even storing the name of the stitiched up VRT file:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "* Put everything together as a function\n",
    "* Mask pixels outside the Netherlands\n",
    "* Mask pixels outside Ireland\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mosaic_and_mask_data(gdal_fnames, vector_file, vector_where):\n",
    "    stitch_vrt = gdal.BuildVRT(\"\", gdal_fnames)\n",
    "    g = gdal.Warp(\"\", stitch_vrt,\n",
    "                 format = 'MEM', dstNodata=200,\n",
    "                  cutlineDSName = vector_file,\n",
    "                  cutlineWhere = vector_where)\n",
    "\n",
    "    return g\n",
    "\n",
    "g = mosaic_and_mask_data(gdal_fnames, \"data/TM_WORLD_BORDERS-0.3.shp\",\n",
    "                         \"FIPS='UK'\")\n",
    "lai = g.ReadAsArray()\n",
    "mask = np.nonzero(lai < 200)\n",
    "min_y = mask[0].min()\n",
    "max_y = mask[0].max() + 1\n",
    "\n",
    "min_x = mask[1].min()\n",
    "max_x = mask[1].max() + 1\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.imshow( lai[min_y:max_y,\n",
    "               min_x:max_x]/10., vmin=0, vmax=6,\n",
    "           cmap=plt.cm.inferno_r)\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = mosaic_and_mask_data(gdal_fnames, \"data/TM_WORLD_BORDERS-0.3.shp\",\n",
    "                         \"FIPS='NL'\")\n",
    "lai = g.ReadAsArray()\n",
    "mask = np.nonzero(lai < 200)\n",
    "min_y = mask[0].min()\n",
    "max_y = mask[0].max() + 1\n",
    "\n",
    "min_x = mask[1].min()\n",
    "max_x = mask[1].max() + 1\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.imshow( lai[min_y:max_y,\n",
    "               min_x:max_x]/10., vmin=0, vmax=6,\n",
    "           cmap=plt.cm.inferno_r)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": "3.1",
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "47px",
    "left": "0px",
    "top": "110px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
