{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Accessing MODIS Data products using `gdal`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[GDAL](https://gdal.org) is the workhorse of geospatial processing. Basically, GDAL offers a common library to access a vast number of formats (if you want to see how vast, [check this](https://gdal.org/formats_list.html)). In addition to letting you open and convert obscure formats to something more useful, a lot of functionality in terms of processing raster data is available (for example, working with projections, combining datasets, accessing remote datasets, etc).\n",
    "\n",
    "For vector data, the counterpart to GDAL is OGR (which is now a part of the GDAL library anyway), which also supports [many vector formats](https://gdal.org/ogr_formats.html). The combination of both libraries is a very powerful tool to work with geospatial data, not only from Python, but from [many other popular computer languages](https://trac.osgeo.org/gdal/#GDALOGRInOtherLanguages).\n",
    "\n",
    "In this session, we will introduce the `gdal` geospatial module which can read a wide range of raster scientific data formats. We will also introduce the related `ogr` vector package.\n",
    "\n",
    "In pacticular, we will learn how to:\n",
    "\n",
    "* access and download NASA geophysical datasets (specifically, the MODIS LAI/FPAR product)\n",
    "* apply a vector mask to the dataset\n",
    "* apply quality control flags to the data\n",
    "* stack datasets into a 3D numpy dataset for further analysis, including interpolation of missing values\n",
    "* visualise the data\n",
    "* store the stacked dataset\n",
    "\n",
    "**These are all tasks that you will be required to do for the [part 1 formal assessment](Formal_assessment_part1.ipynb) of this course. You will however be using a different NASA dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#MODIS-LAI-product\" data-toc-modified-id=\"MODIS-LAI-product-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>MODIS LAI product</a></span><ul class=\"toc-item\"><li><span><a href=\"#NASA-MODIS-data-access\" data-toc-modified-id=\"NASA-MODIS-data-access-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>NASA MODIS data access</a></span><ul class=\"toc-item\"><li><span><a href=\"#Register-at-NASA-Earthdata\" data-toc-modified-id=\"Register-at-NASA-Earthdata-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Register at NASA Earthdata</a></span></li><li><span><a href=\"#Accessing-NASA-MODIS-URLs\" data-toc-modified-id=\"Accessing-NASA-MODIS-URLs-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Accessing NASA MODIS URLs</a></span></li></ul></li><li><span><a href=\"#MODIS-filename-format\" data-toc-modified-id=\"MODIS-filename-format-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>MODIS filename format</a></span></li><li><span><a href=\"#downloading-the-data-file\" data-toc-modified-id=\"downloading-the-data-file-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>downloading the data file</a></span></li></ul></li><li><span><a href=\"#GDAL\" data-toc-modified-id=\"GDAL-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>GDAL</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Binary-data\" data-toc-modified-id=\"Binary-data-2.0.1\"><span class=\"toc-item-num\">2.0.1&nbsp;&nbsp;</span>Binary data</a></span></li></ul></li></ul></li><li><span><a href=\"#Exercise-4.1\" data-toc-modified-id=\"Exercise-4.1-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Exercise 4.1</a></span></li><li><span><a href=\"#Downloading-data\" data-toc-modified-id=\"Downloading-data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Downloading data</a></span><ul class=\"toc-item\"><li><span><a href=\"#NASA-EarthData\" data-toc-modified-id=\"NASA-EarthData-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>NASA EarthData</a></span></li><li><span><a href=\"#Direct-downloading\" data-toc-modified-id=\"Direct-downloading-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Direct downloading</a></span></li></ul></li><li><span><a href=\"#Exercise-A-Different-Dataset\" data-toc-modified-id=\"Exercise-A-Different-Dataset-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Exercise A Different Dataset</a></span><ul class=\"toc-item\"><li><span><a href=\"#Download\" data-toc-modified-id=\"Download-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Download</a></span></li><li><span><a href=\"#Explore\" data-toc-modified-id=\"Explore-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Explore</a></span></li><li><span><a href=\"#Read-a-dataset\" data-toc-modified-id=\"Read-a-dataset-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Read a dataset</a></span></li><li><span><a href=\"#Water-mask\" data-toc-modified-id=\"Water-mask-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Water mask</a></span></li><li><span><a href=\"#Valid-pixel-mask\" data-toc-modified-id=\"Valid-pixel-mask-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Valid pixel mask</a></span></li><li><span><a href=\"#3D-dataset\" data-toc-modified-id=\"3D-dataset-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;</span>3D dataset</a></span></li></ul></li><li><span><a href=\"#4.3-Vector-masking\" data-toc-modified-id=\"4.3-Vector-masking-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>4.3 Vector masking</a></span></li><li><span><a href=\"#Exercise-4.3\" data-toc-modified-id=\"Exercise-4.3-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Exercise 4.3</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## MODIS LAI product \n",
    "To introduce `gdal` we will use a dataset from the MODIS LAI product over the UK. \n",
    "\n",
    "You should note that the dataset you need to use for your assessed practical is a MODIS dataset with similar characteristics to the one in this example.\n",
    "\n",
    "The data product [MOD16](https://modis.gsfc.nasa.gov/data/dataprod/mod15.php) LAI/FPAR has been generated from NASA MODIS sensors Terra and Aqua data since 2002. We are now in dataset collection 6 (the data version to use).\n",
    "\n",
    "    LAI is defined as the one-sided green leaf area per unit ground area in broadleaf canopies and as half the total needle surface area per unit ground area in coniferous canopies. FPAR is the fraction of photosynthetically active radiation (400-700 nm) absorbed by green vegetation. Both variables are used for calculating surface photosynthesis, evapotranspiration, and net primary production, which in turn are used to calculate terrestrial energy, carbon, water cycle processes, and biogeochemistry of vegetation. Algorithm refinements have improved quality of retrievals and consistency with field measurements over all biomes, with a focus on woody vegetation.\n",
    "    \n",
    "We use such data to map and understand about the dynamics of terrestrial vegetation / carbon, for example, for climate studies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raster data are arranged in tiles, indexed by row and column, to cover the globe:\n",
    "\n",
    "\n",
    "![MODIS tiles](https://www.researchgate.net/profile/J_Townshend/publication/220473201/figure/fig5/AS:277546596880390@1443183673583/The-global-MODIS-Sinusoidal-tile-grid.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.1.1**\n",
    "\n",
    "The pattern on the tile names is `hXXvYY` where `XX` is the horizontal coordinate and `YY` the vertical.\n",
    "\n",
    "\n",
    "* use the map above to work out the names of the two tiles that we will need to access data over the UK\n",
    "* set the variable `tiles` to contain these two names in a list\n",
    "\n",
    "For example, for the two tiles covering Madegascar, we would set:\n",
    "\n",
    "    tiles = ['h22v10','h22v11']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do exercise here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NASA MODIS data access\n",
    "\n",
    "#### Register at NASA Earthdata\n",
    "\n",
    "Before you attempt to do this section, you will need to register at [NASA Earthdata](https://urs.earthdata.nasa.gov/home).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have set up these notes so that you don't have to put your username and password in plain text. Instead, you need to enter your username and password when prompted by `cylog`. The password is stored in an encrypted file, although it can be accessed as plain text within your Python session.\n",
    "\n",
    "**N.B. using `cylog().login()` is only intended to work with access to NASA Earthdata and to prevent you having to expose your username and password in these notes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this seems to be ok ... \n",
      "use cylog().login() anywhere you need to specify the tuple (username,password)\n"
     ]
    }
   ],
   "source": [
    "from cylog import cylog\n",
    "import requests\n",
    "\n",
    "url = 'https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2018.09.30/' \n",
    "        \n",
    "# grab the HTML information\n",
    "html = requests.get(url,auth=cylog().login()).text\n",
    "\n",
    "# test a few lines of the html\n",
    "if html[:20] == '<!DOCTYPE HTML PUBLI':\n",
    "    print('this seems to be ok ... ')\n",
    "    print('use cylog().login() anywhere you need to specify the tuple (username,password)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The NASA servers go down for weekly maintenance, usually on Wednesday afternoon (UK time), so you might not want to attempt this exercise then. If so, skip the rest of this section and do it another time.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Accessing NASA MODIS URLs\n",
    "\n",
    "\n",
    "Before dealing into the data, we will first learn something of how to *automatically* access such data. Since we might often want to work with a large number of files (e.g. for analysing LAI or other variables over space/time), we will want to write code that allows us to do this. \n",
    "\n",
    "If the data we want to use are accessible to us as a URL, we can simply use `requests` as in previous exercises.\n",
    "\n",
    "Sometimes, we will be able to specify the parameters of the dataset we want, e.g. using [JSON](https://www.json.org). At othertimes (as in the case here) we might need to do a little work ourselves to construct the particular URL we want.\n",
    "\n",
    "If you visit the site [https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006](https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006), you will see 'date' style links (e.g. `2018.09.30`) through to sub-directories. \n",
    "\n",
    "In these, e.g. [https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2018.09.30/](https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2018.09.30/) you will find URLs of a set of files. \n",
    "\n",
    "The files pointed to by the URLs are the MODIS MOD15 4-day composite 500 m LAI/FPAR product [MCD15A3H](https://lpdaac.usgs.gov/dataset_discovery/modis/modis_products_table/mcd15a3h_v006).\n",
    "\n",
    "There are links to several datasets on the page, including 'quicklook files' that are jpeg format images of the datasets, e.g.:\n",
    "\n",
    "![MCD15A3H.A2018273.h17v03](https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2018.09.30/BROWSE.MCD15A3H.A2018273.h17v03.006.2018278143630.1.jpg)\n",
    "\n",
    "as well as `xml` files and `hdf` datasets. \n",
    "\n",
    "When we access this 'listing' (directory links such as [https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2018.09.30/](https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2018.09.30/)) from Python, we will obtain the information in [HTML](https://www.w3schools.com/html/). We don't expect you to know this language, but knowing some of the basics is oftem useful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\">\n",
      "<html>\n",
      " <head>\n",
      "  <title>Index of /MOTA/MCD15A3H.006/2018.09.30</title>\n",
      " </head>\n",
      " <body>\n",
      "<pre>\n",
      "********************************************************************************\n",
      "\n",
      "                         U.S. GOVERNMENT COMPUTER\n",
      "\n",
      "This US Government computer is for authorized users only.  By accessing this\n",
      "system you are consenting to complete monitoring with no expectation of privacy.\n",
      "Unauthorized access or use may subject you to disciplinary action and criminal\n",
      "prosecution.\n",
      "\n",
      "Attention user: You are downloading data from NASA's Land Processes Distributed\n",
      "Active Archive Center (LP DAAC) located at the USGS Earth Resources Observation and\n",
      "Science (EROS) Center.\n",
      "\n",
      "Downloading these data requires a NASA Earthdata Login username and password.\n",
      "To obtain a NASA Earthdata Login account, please visit\n",
      "<a href=\"https://urs.earthdata.nasa.gov/users/new\">https://urs.earthdata.nasa.gov/users/new/</a>\n",
      "\n",
      " ------------------------------ etc ------------------------------\n",
      "\n",
      "<img src=\"/icons/unknown.gif\" alt=\"[   ]\"> <a href=\"MCD15A3H.A2018273.h35v08.006.2018278143649.hdf.xml\">MCD15A3H.A2018273.h35v08.006.2018278143649.hdf.xml</a>      2018-10-05 09:42  7.6K  \n",
      "<img src=\"/icons/unknown.gif\" alt=\"[   ]\"> <a href=\"MCD15A3H.A2018273.h35v09.006.2018278143649.hdf\">MCD15A3H.A2018273.h35v09.006.2018278143649.hdf</a>          2018-10-05 09:42  207K  \n",
      "<img src=\"/icons/unknown.gif\" alt=\"[   ]\"> <a href=\"MCD15A3H.A2018273.h35v09.006.2018278143649.hdf.xml\">MCD15A3H.A2018273.h35v09.006.2018278143649.hdf.xml</a>      2018-10-05 09:42  7.6K  \n",
      "<img src=\"/icons/unknown.gif\" alt=\"[   ]\"> <a href=\"MCD15A3H.A2018273.h35v10.006.2018278143650.hdf\">MCD15A3H.A2018273.h35v10.006.2018278143650.hdf</a>          2018-10-05 09:42  298K  \n",
      "<img src=\"/icons/unknown.gif\" alt=\"[   ]\"> <a href=\"MCD15A3H.A2018273.h35v10.006.2018278143650.hdf.xml\">MCD15A3H.A2018273.h35v10.006.2018278143650.hdf.xml</a>      2018-10-05 09:42  7.6K  \n",
      "<hr></pre>\n",
      "</body></html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from cylog import cylog\n",
    "\n",
    "\n",
    "url = 'https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2018.09.30/' \n",
    "        \n",
    "# grab the HTML information\n",
    "# use auth=cylog().login() here instead of (username,password)\n",
    "\n",
    "html = requests.get(url,auth=cylog().login()).text\n",
    "\n",
    "# print a few lines of the html\n",
    "print(html[:951])\n",
    "# etc\n",
    "print('\\n','-'*30,'etc','-'*30)\n",
    "# at the end\n",
    "print(html[-964:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In HTML the code: \n",
    "\n",
    "    <a href=\"MCD15A3H.A2018273.h35v10.006.2018278143650.hdf\">MCD15A3H.A2018273.h35v10.006.2018278143650.hdf</a>  \n",
    "\n",
    "\n",
    "specifies an HTML link, that will appear as \n",
    "\n",
    "    MCD15A3H.A2018273.h35v10.006.2018278143650.hdf 2018-10-05 09:42  7.6K \n",
    "    \n",
    "and link to the URL specified in the `href` field: `MCD15A3H.A2018273.h35v10.006.2018278143650.hdf`.\n",
    "\n",
    "We could interpret this information by searching for strings etc., but the package `BeautifulSoup` can help us a lot in this.\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://urs.earthdata.nasa.gov/users/new', 'https://lpdaac.usgs.gov', '?C=N;O=D', '?C=M;O=A', '?C=S;O=A', '?C=D;O=A', '/MOTA/MCD15A3H.006/', 'BROWSE.MCD15A3H.A2018273.h00v08.006.2018278143557.1.jpg', 'BROWSE.MCD15A3H.A2018273.h00v08.006.2018278143557.2.jpg', 'BROWSE.MCD15A3H.A2018273.h00v09.006.2018278143556.1.jpg']\n",
      "\n",
      " ------------------------------ etc ------------------------------ \n",
      "\n",
      "['MCD15A3H.A2018273.h34v09.006.2018278143649.hdf', 'MCD15A3H.A2018273.h34v09.006.2018278143649.hdf.xml', 'MCD15A3H.A2018273.h34v10.006.2018278143649.hdf', 'MCD15A3H.A2018273.h34v10.006.2018278143649.hdf.xml', 'MCD15A3H.A2018273.h35v08.006.2018278143649.hdf', 'MCD15A3H.A2018273.h35v08.006.2018278143649.hdf.xml', 'MCD15A3H.A2018273.h35v09.006.2018278143649.hdf', 'MCD15A3H.A2018273.h35v09.006.2018278143649.hdf.xml', 'MCD15A3H.A2018273.h35v10.006.2018278143650.hdf', 'MCD15A3H.A2018273.h35v10.006.2018278143650.hdf.xml']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from cylog import cylog\n",
    "\n",
    "\n",
    "url = 'https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2018.09.30/' \n",
    "html = requests.get(url,auth=cylog().login()).text\n",
    "\n",
    "# use BeautifulSoup\n",
    "# to get all urls referenced with\n",
    "# html code <a href=\"some_url\">\n",
    "soup = BeautifulSoup(html,'lxml')\n",
    "links = [mylink.attrs['href'] for mylink in soup.find_all('a')]\n",
    "\n",
    "# print a few lines of the links\n",
    "print(links[:10])\n",
    "# etc\n",
    "print('\\n','-'*30,'etc','-'*30,'\\n')\n",
    "# at the end\n",
    "print(links[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise E3.1.2**\n",
    "\n",
    "* copy the code in the block above up until the `print` statements\n",
    "* using a `for ... in ...:` loop and an `if ... :` statement (or better still, an implicit loop), make a list called `hdf_filenames` of only those filenames (links) that have `hdf` as their filename extension.\n",
    "\n",
    "**Hint 1**: first select an example item from the `links` list: \n",
    "\n",
    "    item = links[-1]\n",
    "    print('item is',item)\n",
    "    \n",
    "and print:\n",
    "\n",
    "    item[-3:]\n",
    "        \n",
    "but maybe better (why would this be?) is:\n",
    "\n",
    "    item.split('.')[-1]\n",
    "    \n",
    "**Hint 2**: An implicit loop is a construct of the form:\n",
    "\n",
    "    [item for item in links]\n",
    "\n",
    "In an implicit for loop, we can actually add a conditional statement if we like, e.g. try:\n",
    "\n",
    "    hdf_filenames = [item for item in links if item[-5] == '4']\n",
    "    \n",
    "This will print out `item` if the condition `item[-5] == '4'` is met. That's a bit of a pointless test, but illustrates the pattern required. Try this now with the condition you want to use to select `hdf` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MCD15A3H.A2018273.h01v07.006.2018278143554.hdf',\n",
       " 'MCD15A3H.A2018273.h01v10.006.2018278143554.hdf',\n",
       " 'MCD15A3H.A2018273.h09v03.006.2018278143554.hdf',\n",
       " 'MCD15A3H.A2018273.h10v04.006.2018278143614.hdf',\n",
       " 'MCD15A3H.A2018273.h11v10.006.2018278143614.hdf',\n",
       " 'MCD15A3H.A2018273.h13v12.006.2018278143604.hdf',\n",
       " 'MCD15A3H.A2018273.h13v13.006.2018278143604.hdf',\n",
       " 'MCD15A3H.A2018273.h16v05.006.2018278143634.hdf',\n",
       " 'MCD15A3H.A2018273.h16v08.006.2018278143634.hdf',\n",
       " 'MCD15A3H.A2018273.h19v02.006.2018278143634.hdf',\n",
       " 'MCD15A3H.A2018273.h19v10.006.2018278143644.hdf',\n",
       " 'MCD15A3H.A2018273.h20v05.006.2018278143634.hdf',\n",
       " 'MCD15A3H.A2018273.h21v07.006.2018278143644.hdf',\n",
       " 'MCD15A3H.A2018273.h22v04.006.2018278143644.hdf',\n",
       " 'MCD15A3H.A2018273.h23v08.006.2018278143634.hdf',\n",
       " 'MCD15A3H.A2018273.h25v05.006.2018278143644.hdf',\n",
       " 'MCD15A3H.A2018273.h26v03.006.2018278143634.hdf',\n",
       " 'MCD15A3H.A2018273.h27v09.006.2018278143634.hdf',\n",
       " 'MCD15A3H.A2018273.h28v09.006.2018278143634.hdf',\n",
       " 'MCD15A3H.A2018273.h28v14.006.2018278143634.hdf',\n",
       " 'MCD15A3H.A2018273.h30v05.006.2018278143634.hdf',\n",
       " 'MCD15A3H.A2018273.h30v13.006.2018278143634.hdf',\n",
       " 'MCD15A3H.A2018273.h32v07.006.2018278143634.hdf',\n",
       " 'MCD15A3H.A2018273.h33v10.006.2018278143644.hdf']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do exercise here\n",
    "[item for item in links if item[-5] == '4']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODIS filename format\n",
    "\n",
    "The `hdf` filenames are of the form:\n",
    "\n",
    "    MCD15A3H.A2018273.h35v10.006.2018278143650.hdf\n",
    "    \n",
    "where:\n",
    "\n",
    "* the first field (`MCD15A3H`) gives the product code\n",
    "* the second (`A2018273`) gives the observation date: day of year `273`, `2018` here\n",
    "* the third (`h35v10`) gives the 'MODIS tile' code for the data location\n",
    "* the remaining fields specify the product version number (`006`) and a code representing the processing date.\n",
    "\n",
    "If we want a particular dataset, we would assume then that we know the information to construct the first four fields.\n",
    "\n",
    "We then have the task remaining of finding an address of the pattern:\n",
    "\n",
    "    MCD15A3H.A2018273.h17v03.006.*.hdf\n",
    "    \n",
    "where `*` represents a wildcard (unknown element of the URL/filename).\n",
    "\n",
    "Putting together the code from above to get a list of the `hdf` files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MCD15A3H.A2018273.h00v08.006.2018278143557.hdf',\n",
       " 'MCD15A3H.A2018273.h00v09.006.2018278143556.hdf',\n",
       " 'MCD15A3H.A2018273.h00v10.006.2018278143557.hdf',\n",
       " 'MCD15A3H.A2018273.h01v07.006.2018278143554.hdf',\n",
       " 'MCD15A3H.A2018273.h01v08.006.2018278143557.hdf',\n",
       " 'MCD15A3H.A2018273.h01v09.006.2018278143558.hdf',\n",
       " 'MCD15A3H.A2018273.h01v10.006.2018278143554.hdf',\n",
       " 'MCD15A3H.A2018273.h01v11.006.2018278143555.hdf',\n",
       " 'MCD15A3H.A2018273.h02v06.006.2018278143556.hdf',\n",
       " 'MCD15A3H.A2018273.h02v08.006.2018278143557.hdf']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from cylog import cylog\n",
    "\n",
    "url = 'https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2018.09.30/' \n",
    "html = requests.get(url,auth=cylog().login()).text\n",
    "links = [mylink.attrs['href'] for mylink in BeautifulSoup(html,'lxml').find_all('a')]\n",
    "\n",
    "# get all files that end 'hdf' as in example above\n",
    "hdf_filenames = [item for item in links if item.split('.')[-1] == 'hdf']\n",
    "# print some out\n",
    "hdf_filenames[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to specify a particular tile or tiles to access.\n",
    "\n",
    "In this case, we want to look at the field `item.split('.')[-4]` and check to see if it is the list `tiles`.\n",
    "\n",
    "**Exercise 3.1.3**\n",
    "\n",
    "First, let's check what we get when we look at `item.split('.')[-4]`.\n",
    "\n",
    "* set a variable called `tiles` containing the names of the UK tiles (as in Exercise 3.1.1)\n",
    "* write a loop `for item in links:` to loop over each item in the list `links`\n",
    "* inside this loop set the condition `if item.split('.')[-1] == 'hdf':` to select only `hdf` files, as above\n",
    "* inside this conditional statement, print out `item.split('.')[-4]` to see if it looks like the tile names\n",
    "* having confirmed that you are getting the right information, add another conditional statement to see if `item.split('.')[-4] in tiles`, and then print only those filenames that pass both of your tests\n",
    "* see if you can combine the two tests (the two `if` statements) into a single one\n",
    "\n",
    "**Hint 1**: if you print all of the tilenames, this will go on for quite some time. Instead it may be better to use `print(item.split('.')[-4],end=' ')`, which will put a space, rather than a newline between each item printed.\n",
    "\n",
    "**Hint 2**: recall what the logical statement `(A and B)` gives when thinking about the combined `if` statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do exercise here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MCD15A3H.A2018273.h17v03.006.2018278143630.hdf', 'MCD15A3H.A2018273.h18v03.006.2018278143633.hdf']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from cylog import cylog\n",
    "\n",
    "tiles = ['h17v03', 'h18v03']\n",
    "destination_folder = 'data'\n",
    "url = 'https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2018.09.30/' \n",
    "\n",
    "html = requests.get(url,auth=cylog().login()).text\n",
    "links = [mylink.attrs['href'] for mylink in BeautifulSoup(html,'lxml').find_all('a')]\n",
    "\n",
    "# get all files that end 'hdf' as in example above\n",
    "hdf_filenames = [item for item in links if item.split('.')[-1] == 'hdf']\n",
    "tile_filenames = [item for item in hdf_filenames if item.split('.')[-4] in tiles]\n",
    "\n",
    "# loop over tile filenames\n",
    "print(tile_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise E3.1.4**\n",
    "\n",
    "Most of the code above is generic in nature and it would make sense at this point to write a *function* that includes this.\n",
    "\n",
    "In the code box below, you are given the basic structure of such a function.\n",
    "\n",
    "* then, take the code from above and use it to complete the function. At this point, it should only return the filenames, as in the code segment above.\n",
    "* improve the description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# do exercise here\n",
    "#\n",
    "\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "from cylog import cylog\n",
    "\n",
    "def modis_tiles(tiles=['h17v03', 'h18v03'],\\\n",
    "                 destination_folder = 'data',\\\n",
    "                 url='https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2018.09.30/'):\n",
    "    \"\"\"Put in a description here\n",
    "       \n",
    "    Parameters\n",
    "    ----------\n",
    "    tiles: list\n",
    "        List of strings of MODIS product tilenames\n",
    "    destination_folder: str\n",
    "        The destination folder\n",
    "    url: str\n",
    "        The required URL\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    A string with the location of the downloaded file.\n",
    "    \"\"\"\n",
    "    output_fname = None\n",
    "    ## code here !!!\n",
    "    return output_fname\n",
    "\n",
    "# test:\n",
    "print(modis_tiles())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### downloading the data file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now form the full url of the dataset:\n",
    "\n",
    "    for filename in tile_filenames:  \n",
    "        full_url = url+filename\n",
    "        \n",
    "We suppose that we want to save the dataset to a local file on the system.\n",
    "\n",
    "It makes sense here to give the file the full MODIS filename. We need only then specify a directory ('folder') that we want to put the dataset in.\n",
    "\n",
    "We set this to be `data` here. Before we go any further we should check:\n",
    "\n",
    "* that the directory exists (if not, create it)\n",
    "* that the file we want to download doesn't already exist (else, don't bother)\n",
    "\n",
    "We can conveniently use methods in [`pathlib.Path`](https://docs.python.org/3/library/pathlib.html) for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function uses the `requests` library to pull the data from the URL and save it to a local file.\n",
    "\n",
    "You don't need to be able to write code of this complexity at the moment, though you might find it interesting to look at and work out what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/MCD15A3H.A2018273.h17v03.006.2018278143630.hdf'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "from cylog import cylog\n",
    "\n",
    "def save_data(url,filename,destination_folder):\n",
    "    \"\"\"Downloads hdf data from a NASA URL for the specified tiles\n",
    "       and saves the files in destination_folder.\n",
    "       \n",
    "       Checks to see if destination_folder exists, if not,\n",
    "       creates it.\n",
    "       \n",
    "       Could do with further error checking\n",
    "       \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename: string\n",
    "        MODIS product filename\n",
    "    destination_folder: str\n",
    "        The destination folder\n",
    "    url: str\n",
    "        The required URL\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    A string with the location of the downloaded file.\n",
    "    \"\"\"\n",
    "    # make sure destination_folder exists\n",
    "    dest_path = Path(destination_folder)\n",
    "    if not dest_path.exists():\n",
    "        dest_path.mkdir()\n",
    "    \n",
    "    # make a compound file name from folder and filename\n",
    "    output_fname = dest_path.joinpath(filename)\n",
    "    \n",
    "    # does the file already exist?\n",
    "    if not output_fname.exists():\n",
    "        # put download and save code here\n",
    "        with requests.Session() as session:\n",
    "            # get password-authorised url\n",
    "            session.auth = cylog().login()\n",
    "            r1 = session.request('get',url+filename)\n",
    "            r2 = session.get(r1.url)\n",
    "            with open(output_fname, 'wb') as fp:\n",
    "                r = fp.write(r2.content)\n",
    "            \n",
    "    return str(output_fname)\n",
    "\n",
    "\n",
    "# set up an example\n",
    "url      = 'https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2018.09.30/' \n",
    "filename = 'MCD15A3H.A2018273.h17v03.006.2018278143630.hdf'\n",
    "destination_folder = 'data'\n",
    "\n",
    "save_data(url,filename,destination_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise E3.1.5**\n",
    "\n",
    "* make use of the function `save_data` to complete your function `modis_tiles` so that it loops over the tiles you request and dowenloads the data. It should return a list of the output filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do exercise here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.1.6 Homework**\n",
    "\n",
    "* There's a lot more that could be done with `save_data()`. Spend some time thinking this through and try to come up with an improved function.\n",
    "\n",
    "**Hint**: for example, when you have the filename e.g.`MCD15A3H.A2018273.h17v03.006.2018278143630.hdf`, you should no longer have to specify the 'full' URL `https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2018.09.30/`. Instead, you should be able to give it as `https://e4ftl01.cr.usgs.gov/MOTA` and work out the rest from the filename. That would involve converting the year and day of year from the filename (`2018`, `273` here) into a date string (`2018.09.30` here). It is probably easiest to use the [`datetime`](https://docs.python.org/3/library/datetime.html) module for this once you have pulled out `year` and `doy` as integers:\n",
    "\n",
    "    import datetime\n",
    "\n",
    "    d = datetime.datetime(year,1,1) + datetime.timedelta(doy-1)\n",
    "    datestr = f'{d.year}.{d.month}.{d.day}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GDAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic operation of `gdal` involves:\n",
    "\n",
    "- load the gdal module\n",
    "- open a spatial dataset (an hdf format file here)\n",
    "- specify which subsets you want.\n",
    "\n",
    "We can explore the subsets in the file with `GetSubDatasets()`\n",
    "\n",
    "First, let's make sure we have some suitable datasets for this session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running outside UCL Geography. Will need to download data. This might take a while!\n",
      "trying geog server ...\n",
      "trying NASA server ...\n",
      "file data/MCD15A3H.A2018273.h17v03.006.2018278143630.hdf exists\n",
      "Running outside UCL Geography. Will need to download data. This might take a while!\n",
      "trying geog server ...\n",
      "trying NASA server ...\n",
      "file data/MCD15A3H.A2018273.h18v03.006.2018278143633.hdf exists\n"
     ]
    }
   ],
   "source": [
    "from geog_data import procure_dataset\n",
    "\n",
    "tile_filenames = ['MCD15A3H.A2018273.h17v03.006.2018278143630.hdf', 'MCD15A3H.A2018273.h18v03.006.2018278143633.hdf']\n",
    "for f in tile_filenames:\n",
    "    procure_dataset(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCD15A3H.A2018273.h17v03.006.2018278143630.hdf\n",
      "**********************************************\n",
      "[2400x2400] Fpar_500m MOD_Grid_MCD15A3H (8-bit unsigned integer)\n",
      "\t HDF4_EOS:EOS_GRID:\"data/MCD15A3H.A2018273.h17v03.006.2018278143630.hdf\":MOD_Grid_MCD15A3H:Fpar_500m\n",
      "[2400x2400] Lai_500m MOD_Grid_MCD15A3H (8-bit unsigned integer)\n",
      "\t HDF4_EOS:EOS_GRID:\"data/MCD15A3H.A2018273.h17v03.006.2018278143630.hdf\":MOD_Grid_MCD15A3H:Lai_500m\n",
      "[2400x2400] FparLai_QC MOD_Grid_MCD15A3H (8-bit unsigned integer)\n",
      "\t HDF4_EOS:EOS_GRID:\"data/MCD15A3H.A2018273.h17v03.006.2018278143630.hdf\":MOD_Grid_MCD15A3H:FparLai_QC\n",
      "[2400x2400] FparExtra_QC MOD_Grid_MCD15A3H (8-bit unsigned integer)\n",
      "\t HDF4_EOS:EOS_GRID:\"data/MCD15A3H.A2018273.h17v03.006.2018278143630.hdf\":MOD_Grid_MCD15A3H:FparExtra_QC\n",
      "[2400x2400] FparStdDev_500m MOD_Grid_MCD15A3H (8-bit unsigned integer)\n",
      "\t HDF4_EOS:EOS_GRID:\"data/MCD15A3H.A2018273.h17v03.006.2018278143630.hdf\":MOD_Grid_MCD15A3H:FparStdDev_500m\n",
      "[2400x2400] LaiStdDev_500m MOD_Grid_MCD15A3H (8-bit unsigned integer)\n",
      "\t HDF4_EOS:EOS_GRID:\"data/MCD15A3H.A2018273.h17v03.006.2018278143630.hdf\":MOD_Grid_MCD15A3H:LaiStdDev_500m\n"
     ]
    }
   ],
   "source": [
    "import gdal \n",
    "from pathlib import Path\n",
    "\n",
    "destination_folder = 'data'\n",
    "filename = 'MCD15A3H.A2018273.h17v03.006.2018278143630.hdf'\n",
    "\n",
    "dest_path = Path(destination_folder)\n",
    "print (filename)\n",
    "print ('*'*len(filename))\n",
    "\n",
    "fname = str(dest_path.joinpath(filename))\n",
    "# open dataset\n",
    "g = gdal.Open(fname)\n",
    "if g != None:\n",
    "    subdatasets = g.GetSubDatasets()\n",
    "    for fname, name in subdatasets:\n",
    "        print (name)\n",
    "        print (\"\\t\", fname)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous code snippet we have done a number of different things:\n",
    "\n",
    "1. Import the GDAL library\n",
    "2. Open a file with GDAL, storing a handler to the file in `g`\n",
    "3. Test that `g` is not `None` (as this indicates failure opening the file. Try changing `filename` above to something else)\n",
    "4. We then use the `GetSubDatasets()` method to read out information on the different subdatasets available from this file \n",
    "5. Loop over the retrieved subdatasets to print the name (human-readable information) and the GDAL filename. This last item is the filename that you need to use to tell GDAL to open a particular data layer of the 6 layers present in this example\n",
    "\n",
    "Let's say that we want to access the LAI information. By contrasting the output of the above code (or `gdalinfo`) to the contents of the [LAI/fAPAR product information page](https://lpdaac.usgs.gov/dataset_discovery/modis/modis_products_table/mcd15a3h), we find out that we want the layers for `Lai_500m`, `FparLai_Qc`, `FparExtra_QC` and `LaiStdDev_500m`. \n",
    "\n",
    "To read these individual datasets, we need to open each of them individually using GDAL, and the GDAL filenames used above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-31T15:13:12.085511Z",
     "start_time": "2017-10-31T15:13:12.029029Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening Layer 1: HDF4_EOS:EOS_GRID:\"data/MCD15A3H.A2018273.h17v03.006.2018278143630.hdf\":MOD_Grid_MOD15A3H:Lai_500m\n",
      "\t>>> Read Lai_500m!\n",
      "Opening Layer 2: HDF4_EOS:EOS_GRID:\"data/MCD15A3H.A2018273.h17v03.006.2018278143630.hdf\":MOD_Grid_MOD15A3H:FparLai_QC\n",
      "\t>>> Read FparLai_QC!\n",
      "Opening Layer 3: HDF4_EOS:EOS_GRID:\"data/MCD15A3H.A2018273.h17v03.006.2018278143630.hdf\":MOD_Grid_MOD15A3H:LaiStdDev_500m\n",
      "\t>>> Read LaiStdDev_500m!\n"
     ]
    }
   ],
   "source": [
    "import gdal \n",
    "from pathlib import Path\n",
    "\n",
    "# How to access specific datasets in gdal\n",
    "filename = 'MCD15A3H.A2018273.h17v03.006.2018278143630.hdf'\n",
    "destination_folder = 'data'\n",
    "\n",
    "fname = str(dest_path.joinpath(filename))\n",
    "\n",
    "# Let's create a list with the selected layer names\n",
    "selected_layers = [  \"Lai_500m\", \"FparLai_QC\", \"LaiStdDev_500m\" ]\n",
    "\n",
    "# We will store the data in a dictionary\n",
    "# Initialise an empty dictionary\n",
    "data = {}\n",
    "\n",
    "# for convenience, we will use string substitution to create a \n",
    "# template for GDAL filenames, which we'll substitute on the fly:\n",
    "file_template = 'HDF4_EOS:EOS_GRID:\"{}\":MOD_Grid_MOD15A3H:{}'\n",
    "# This has two substitutions (the %s parts) which will refer to:\n",
    "# - the filename\n",
    "# - the data layer\n",
    "\n",
    "for i, layer in enumerate ( selected_layers ):\n",
    "    this_file = file_template.format( fname, layer )\n",
    "    print (\"Opening Layer %d: %s\" % (i+1, this_file ))\n",
    "    g = gdal.Open ( this_file )\n",
    "    \n",
    "    if g is None:\n",
    "        raise IOError\n",
    "    data[layer] = g.ReadAsArray() \n",
    "    print (\"\\t>>> Read %s!\" % layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous code, we have seen a way of neatly creating the filenames required by GDAL to access the independent datasets: a template string that gets substituted with the `fname` and the `layer` name. Note that the presence of double quotes in the template requires us to use single quotes around it. The data is now stored in a dictionary, and can be accessed as e.g. `data['Lai_500m']` which is a numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-31T15:13:12.538249Z",
     "start_time": "2017-10-31T15:13:12.529690Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data['Lai_500m'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-31T15:13:12.561487Z",
     "start_time": "2017-10-31T15:13:12.541049Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDF4_EOS:EOS_GRID:\"data/MCD15A3H.A2018273.h17v03.006.2018278143630.hdf\":MOD_Grid_MOD15A3H:LaiStdDev_500m\n",
      "<osgeo.gdal.Dataset; proxy of <Swig Object of type 'GDALDatasetShadow *' at 0x12406ee10> >\n"
     ]
    }
   ],
   "source": [
    "print(this_file)\n",
    "g = gdal.Open ( this_file )\n",
    "g.ReadAsArray() \n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.RasterXSize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to translate the LAI values into meaningful quantities. According to the [LAI](https://lpdaac.usgs.gov/products/modis_products_table/leaf_area_index_fraction_of_photosynthetically_active_radiation/8_day_l4_global_1km/mod15a2) webpage, there is a scale factor of 0.1 involved for LAI and SD LAI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-31T15:13:13.090063Z",
     "start_time": "2017-10-31T15:13:13.074115Z"
    }
   },
   "outputs": [],
   "source": [
    "lai = data['Lai_1km'] * 0.1\n",
    "lai_sd = data['LaiStdDev_1km'] * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-31T15:13:13.104666Z",
     "start_time": "2017-10-31T15:13:13.092691Z"
    }
   },
   "outputs": [],
   "source": [
    "print \"LAI\"\n",
    "print lai\n",
    "print \"SD\"\n",
    "print lai_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-31T15:13:13.654079Z",
     "start_time": "2017-10-31T15:13:13.107691Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot the LAI\n",
    "\n",
    "import pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# colormap\n",
    "cmap = plt.cm.Greens\n",
    "\n",
    "plt.imshow(lai,interpolation='none',vmin=0.1,vmax=4.,cmap=cmap)\n",
    "plt.title('MODIS LAI data: DOY 185 2011')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-31T15:13:14.202450Z",
     "start_time": "2017-10-31T15:13:13.656939Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot the LAI std\n",
    "\n",
    "import pylab as plt\n",
    "\n",
    "# colormap\n",
    "cmap = plt.cm.spectral\n",
    "# this sets the no data colour. 'k' is black\n",
    "\n",
    "plt.imshow(lai_sd,interpolation='none',vmax=1.,cmap=cmap)\n",
    "plt.title('MODIS LAI STD data: DOY 185 2011')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not possible to produce LAI estimates if it is persistently cloudy, so the dataset may contain some gaps.\n",
    "\n",
    "These are identified in the dataset using the QC (Quality Control) information.\n",
    "\n",
    "We should then examine this. \n",
    "\n",
    "The codes for this are also given on the LAI product page. They are described as bit combinations:\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th>Bit No.</th>\t<th>Parameter Name</th><th>\tBit Combination</th><th>Explanation</th>\n",
    "<tr>\n",
    "<td>0\t</td><td>MODLAND_QC bits</td><td>\t0</td><td>\tGood quality (main algorithm with or without saturation)\t \t </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>&nbsp;</td><td>&nbsp;</td><td>\t1\t</td><td>Other Quality (back-up algorithm or fill values)\t \t </td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>1\t</td><td>Sensor</td><td>\t0</td><td>\tTERRA</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>&nbsp;</td><td>&nbsp;</td><td>\t1\t</td><td>AQUA</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>2\t</td><td>DeadDetector</td><td>\t0</td><td>\tDetectors apparently fine for up to 50% of channels 1\t2\t </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>&nbsp;</td><td>&nbsp;</td><td>\t1\t</td><td>Dead detectors caused >50% adjacent detector retrieval</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>3-4</td><td>CloudState</td><td>\t00</td><td>\tSignificant clouds NOT present (clear)\t \t </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>&nbsp;</td><td>&nbsp;</td><td>\t01\t</td><td>Significant clouds WERE present</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>&nbsp;</td><td>&nbsp;</td><td>\t10\t</td><td>Mixed clouds present on pixel</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>&nbsp;</td><td>&nbsp;</td><td>\t11\t</td><td>Cloud state not defined assumed clear</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>5-7</td><td>CF_QC</td><td>\t000</td><td>\tMain (RT) method used\tbest result possible (no saturation)\t </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>&nbsp;</td><td>&nbsp;</td><td>\t001\t</td><td>Main (RT) method used with saturation. Good\tvery usable</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>&nbsp;</td><td>&nbsp;</td><td>\t010\t</td><td>Main (RT) method failed due to bad geometry\tempirical algorithm used</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>&nbsp;</td><td>&nbsp;</td><td>\t011\t</td><td> Main (RT) method failed due to problems other than geometry\tempirical algorithm used</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>&nbsp;</td><td>&nbsp;</td><td>\t100\t</td><td> Pixel not produced at all\tvalue coudn’t be retrieved (possible reasons: bad L1B data\tunusable MODAGAGG data)</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "In using this information, it is up to the use which data he/she wants to pass through for any further processing. There are clearly trade-offs: if you look for only the highest quality data, then the number of samples is likely to be lower than if you were more tolerant. But if you are too tolerant, you will get spurious results. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But let's just say that we want to use only the highest quality data. \n",
    "\n",
    "This means we want bit 0 to be 0 ...\n",
    "\n",
    "Let's have a look at the QC data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-31T15:13:14.595458Z",
     "start_time": "2017-10-31T15:13:14.589185Z"
    }
   },
   "outputs": [],
   "source": [
    "qc = data['FparLai_QC'] # Get the QC data which is an unsigned 8 bit byte\n",
    "print qc , qc.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see various byte values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-31T15:13:15.184114Z",
     "start_time": "2017-10-31T15:13:15.140787Z"
    }
   },
   "outputs": [],
   "source": [
    "np.unique(qc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-31T15:13:15.229117Z",
     "start_time": "2017-10-31T15:13:15.186375Z"
    }
   },
   "outputs": [],
   "source": [
    "# translated into binary using bin()\n",
    "for i in np.unique(qc):\n",
    "    print i,bin(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary data\n",
    "\n",
    "Computers store data in base 2, rather than the more usual base 10. A byte contains 8 bits, which can either be 0 or 1. A number is made up of the sum of powers of two that are given a 1. The diagram below shows how the number 53 in base 10 is written as as 00110101 in base 2 (using 8 bits):\n",
    "\n",
    "![Taken from[http://dustlayer.com/cpu-6510-articles/2013/4/18/math-basics-converting-numbering-systems](http://dustlayer.com/cpu-6510-articles/2013/4/18/math-basics-converting-numbering-systems)](https://static1.squarespace.com/static/511651d6e4b0a31c035e30aa/t/51713d9ce4b02974eba2db13/1366375836670/dustlayer.com-binary-to-decimal.png)\n",
    "\n",
    "In the case of QA flags, the ability to indicate a yes/no or a small set of options (e.g. \"Very good\", \"Good\", \"Bad\" \"Terrible\") by position results in an efficient way to convey a lot of information. In the case of the LAI QA product, we have that number 53 (00110101) can be explained as\n",
    "\n",
    "<dl>\n",
    "    <dt>Bit 0 (rightmost bit) = 1</dt><dd> \"Other\" quality</dd>\n",
    "    <dt>Bit 1 = 0 </dt><dd> Sensor is \"TERRA\"</dd>    \n",
    "    <dt>Bit 2 = 1 </dt><dd> Dead detectors</dd>    \n",
    "    <dt>Bits 3 and 4 = 10 </dt><dd>Mixed clouds present on pixel</dd>    \n",
    "    <dt>Bit 5, 6 and 7 = 001 </dt><dd>Main (RT) method used with saturation. Good very usable</dd>    \n",
    "</dl>\n",
    "\n",
    "So we have encoded 5 very different and qualitative pieces of information in a very small amount of storage. In some cases, we might just be interested in checking one or a few of the flags. A simple way to achieve this is to use a logical AND (`&`) operation, where one would `AND` a template binary mask with the number. For example, if we wanted the third bit, we could use the number 0000100 (e.g. decimal 4):\n",
    "\n",
    "    00110101 & 0000100 = 0000100\n",
    "    53 & 4 = 4\n",
    "    \n",
    "This gives us the wanted bit in position 3 (so we can either get 4 or 0). We can then *shift* this number of the right by 3 units to have a 0 or a 1. In Python, we can do this by the binary right shift operator, `>>`:\n",
    "\n",
    "    0000100 >> 2 = 00000001\n",
    "    (53 & 4) >> 2 = 1\n",
    "    \n",
    "When operating on Numpy arrays, one has to operate with [special functions](https://docs.scipy.org/doc/numpy-1.13.0/reference/routines.bitwise.html) ``np.bitwise_and`` and ``np.right_shift``, which will operate on an element by element basis. \n",
    "\n",
    "Let's see what the different fields we found in the LAI product QA flagsfields mean by \n",
    "\n",
    "1. Finding the unique values in the `qc` array\n",
    "2. Looping over them\n",
    "3. Calculate the `AND` of the flag and a mask (in this case, we'll check bits 5-7 and bit 0)\n",
    "4. Shifting the result of the `AND` operation right (if required)\n",
    "\n",
    "We will be printing the results. To print the binary representation of  a number, we can use the format string `#010b` (which uses 0 bits, plus to extra bits for the `0b` at the beginning, and additionally does zero padding). You can read more about [Python string formatters here](https://docs.python.org/2/library/string.html#format-specification-mini-language).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-31T15:13:15.781347Z",
     "start_time": "2017-10-31T15:13:15.738592Z"
    }
   },
   "outputs": [],
   "source": [
    "mask_bits_57 = 0b11100000 # Or decimal 2**7 + 2**6 + 2**5 = 224\n",
    "mask_bit_0 = 0b00000001 # Or decimal 1\n",
    "\n",
    "for value in np.unique(qc):\n",
    "    print \"Value: {:03d}(->{:#010b}), flags 5-7: {:d}(->{:#010b}),flag 0: {:d}(->{:#010b})\".format(\n",
    "            value,\n",
    "            value,\n",
    "            value & mask_bits_57 >> 5, \n",
    "            value & mask_bits_57 >> 5, \n",
    "            value & mask_bit_0, \n",
    "            value & mask_bit_0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for example (examining the table above) `105` is interpreted at `0b011` in fields 5 to 7 (which is 3 in decimal). This indicates that 'Main (RT) method failed due to problems other than geometry empirical algorithm used'. Here, bit zero is set to `1`, so this is a 'bad' pixel.\n",
    "\n",
    "In this case, we are only interested in bit 0, which is an easier task than interpreting all of the bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-31T15:13:16.808483Z",
     "start_time": "2017-10-31T15:13:16.396528Z"
    }
   },
   "outputs": [],
   "source": [
    "# the good data are where qc bit 1 is 0\n",
    "\n",
    "qc = data['FparLai_QC'] # Get the QC data\n",
    "# find bit 0\n",
    "qc = np.bitwise_and(qc, 1)\n",
    "\n",
    "plt.imshow(qc)\n",
    "plt.title('QC bit 1')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this mask to generate a masked array. Masked arrays, as we have seen before, are like normal arrays, but they have an associated mask. \n",
    "\n",
    "Remember that the mask in a masked array should be `False` for good data, so we can directly use `qc` as defined above. \n",
    "\n",
    "We shall also choose another colormap (there are [lots to choose from](http://wiki.scipy.org/Cookbook/Matplotlib/Show_colormaps)), and set values outside the 0.1 and 4 to be shown as black pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-31T15:13:17.654091Z",
     "start_time": "2017-10-31T15:13:17.162938Z"
    }
   },
   "outputs": [],
   "source": [
    "# colormap\n",
    "cmap = plt.cm.Greens\n",
    "cmap.set_bad ( 'k' )\n",
    "# this sets the no data colour. 'k' is black\n",
    "\n",
    "# generate the masked array\n",
    "laim = np.ma.array ( lai, mask=qc )\n",
    "\n",
    "# and plot it\n",
    "plt.imshow ( laim, cmap=cmap, interpolation='none', vmin=0.1, vmax=4)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can do a similar thing for Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-31T15:13:18.401746Z",
     "start_time": "2017-10-31T15:13:17.931606Z"
    }
   },
   "outputs": [],
   "source": [
    "cmap = plt.cm.spectral\n",
    "cmap.set_bad ( 'k' )\n",
    "stdm = np.ma.array ( lai_sd, mask=qc )\n",
    "plt.imshow ( stdm, cmap=cmap, interpolation='none', vmin=0.001, vmax=0.5)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we might wrap all of this up into a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-31T15:13:18.716292Z",
     "start_time": "2017-10-31T15:13:18.694980Z"
    }
   },
   "outputs": [],
   "source": [
    "import gdal\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "\n",
    "\n",
    "def getLAI(filename, \\\n",
    "           qc_layer = 'FparLai_QC',\\\n",
    "           scale = [0.1, 0.1],\\\n",
    "           selected_layers = [\"Lai_1km\", \"LaiStdDev_1km\"]):\n",
    "           \n",
    "    # get the QC layer too\n",
    "    selected_layers.append(qc_layer)\n",
    "    scale.append(1)\n",
    "    # We will store the data in a dictionary\n",
    "    # Initialise an empty dictionary\n",
    "    data = {}\n",
    "    # for convenience, we will use string substitution to create a \n",
    "    # template for GDAL filenames, which we'll substitute on the fly:\n",
    "    file_template = 'HDF4_EOS:EOS_GRID:\"%s\":MOD_Grid_MOD15A2:%s'\n",
    "    # This has two substitutions (the %s parts) which will refer to:\n",
    "    # - the filename\n",
    "    # - the data layer\n",
    "    for i,layer in enumerate(selected_layers):\n",
    "        this_file = file_template % ( filename, layer )\n",
    "        g = gdal.Open ( this_file )\n",
    "        \n",
    "        if g is None:\n",
    "            raise IOError\n",
    "        \n",
    "        data[layer] = g.ReadAsArray() * scale[i]\n",
    "\n",
    "    qc = data[qc_layer] # Get the QC data\n",
    "    # find bit 0\n",
    "    qc = np.bitwise_and( qc, 1)\n",
    "    \n",
    "    odata = {}\n",
    "    for layer in selected_layers[:-1]:\n",
    "        odata[layer] = ma.array(data[layer],mask=qc)\n",
    "    \n",
    "    return odata\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-31T15:13:19.309849Z",
     "start_time": "2017-10-31T15:13:18.719241Z"
    }
   },
   "outputs": [],
   "source": [
    "filename = 'data/MCD15A2.A2011185.h09v05.005.2011213154534.hdf'\n",
    "\n",
    "lai_data = getLAI(filename)\n",
    "\n",
    "# colormap\n",
    "cmap = plt.cm.Greens\n",
    "cmap.set_bad ( 'k' )\n",
    "# this sets the no data colour. 'k' is black\n",
    "\n",
    "# and plot it\n",
    "plt.imshow ( lai_data['Lai_1km'], cmap=cmap, interpolation='nearest', vmin=0.1, vmax=4)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are given the MODIS LAI data files for the year 2012 in the directory `data` for the UK (MODIS tile h17v03).\n",
    "\n",
    "Read these LAI datasets into a masked array, using QA bit 0 to mask the data (i.e. good quality data only) and generate a movie of LAI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should end up with something like:\n",
    "\n",
    "![](files/images/lai_uk02.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading data\n",
    "\n",
    "For the exercise above, you were supplied with the datasets that were previously downloaded. But how would you go about downloading your own data?\n",
    "\n",
    "### NASA EarthData\n",
    "\n",
    "The easiest option would be to use NASA's own system for discovering and accessing data, [EarthData](https://search.earthdata.nasa.gov/search).\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "Go to [EarthData](https://search.earthdata.nasa.gov/search) **NOW** and create a username/password combination\n",
    "** YOU WILL NEED THESE TO DO THE COURSE ASSIGNMENT!**\n",
    "</div>\n",
    "\n",
    "We will be looking for data about snow cover, in particular the MODIS/TERRA snow cover product (product code MOD10A1) covering the UK. You would basically basically search for MOD10A1, select a start and end times, and maybe filter by tile (select `h17v03` in the *Granule Search* box). Then you can click on *Download data*. In the next screen, you would select *Direct Download*. You can then get a Download script that you can download and run (it will ask you for your username and password, and proceed to download the data), but you can also see that the URLs are of the form\n",
    "\n",
    "```\n",
    "https://n5eil01u.ecs.nsidc.org/DP5/MOST/MOD10A1.006/2016.12.15/MOD10A1.A2016350.h17v03.006.2016352154432.hdf\n",
    "```\n",
    "So maybe we can try downloading them with a script?\n",
    "\n",
    "### Direct downloading\n",
    "\n",
    "We have asceartained that the URLs have the following format:\n",
    "```\n",
    "https://n5eil01u.ecs.nsidc.org/DP5/MOST/MOD10A1.006/<date>/<hdf_file>\n",
    "```\n",
    "\n",
    "You can visit any date folder, and see the list of files. We can use the requests module for this. We will also use the `datetime` module to operate with dates. The `datetime` module allows you to work with time easily. In here, we'll be using it to create a date object using `datetime.datetime(year, month, day)`, a time increase (`datetime.timedelta(days=1)`), and to convert from a `datetime` object to a string with a particular format `datetime.strftime(\"%Y.%m.%d\")`, which is the format that corresponds to `2001.12.31`, for example. \n",
    "\n",
    "We can then use requests to loop over each date, get the file listing, and filter it by MODIS tile (e.g. `h17v03` in our case), and select the files that have a `.hdf` extension, ignoring those that also have metadata (`.hdf.xml`).\n",
    "\n",
    "The code to connect to the server and get the data is quite complicated, so we'll use a function provided below called `url_downloader`. You don't need to understand the details of the authentication, but you should be able to figure out what's happening in this bit of the code\n",
    "\n",
    "```python\n",
    "            if r.ok:\n",
    "                for line in r.text.split(\"\\n\"):\n",
    "                    if line.find(\"h17v03\") >= 0 and line.find(\".hdf\") >= 0 and line.find(\".xml\") < 0:\n",
    "                        fname = line.split(\"href\")[1][1:].split('\"')[1]\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-31T15:19:52.187010Z",
     "start_time": "2017-10-31T15:19:47.018185Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import datetime\n",
    "print (\"\"\"\n",
    "########################################################################################\n",
    "###  CHANGE ME! CHANGE ME! CHANGE ME! CHANGE ME! CHANGE ME! CHANGE ME! CHANGE ME!    ###\n",
    "########################################################################################\n",
    "\"\"\")\n",
    "username = \"profLewis\"\n",
    "password = \"GeogG1222016\"\n",
    "\n",
    "def url_downloader (username, password, destination_folder=\"data/\",\n",
    "                    url=\"https://n5eil01u.ecs.nsidc.org/DP5/MOST/MOD10A1.006/2016.01.01/\"):\n",
    "    \n",
    "    \"\"\"Downloads data from a NASA URL, provided that a username/password pair exist.\n",
    "    Parameters\n",
    "    ----------\n",
    "    username: str\n",
    "        The NASA EarthData username\n",
    "    password: str\n",
    "        The NASA EarthData password\n",
    "    destination_folder: str\n",
    "        The destination folder\n",
    "    url: str\n",
    "        The required URL\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    A string with the location of the downloaded file.\n",
    "    \"\"\"\n",
    "    with requests.Session() as session:\n",
    "            session.auth = (username, password)\n",
    "            r1 = session.request('get', url)\n",
    "            r = session.get(r1.url, auth=(username, password))\n",
    "            if r.ok:\n",
    "                for line in r.text.split(\"\\n\"):\n",
    "                    if line.find(\"h17v03\") >= 0 and line.find(\".hdf\") >= 0 and line.find(\".xml\") < 0:\n",
    "                        fname = line.split(\"href\")[1][1:].split('\"')[1]\n",
    "            url_granule = url + fname\n",
    "            r1 = session.request('get', url_granule)\n",
    "            r = session.get(r1.url, auth=(username, password))\n",
    "            output_fname = os.path.join(destination_folder, fname)\n",
    "            if r.ok:\n",
    "                with open(output_fname, 'w') as fp:\n",
    "                    fp.write(r.text)\n",
    "            print (\"Saved file {}\".format(output_fname))\n",
    "            return output_fname\n",
    "\n",
    "filename = url_downloader (username, password)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we just need a loop to get different dates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-31T15:13:52.186052Z",
     "start_time": "2017-10-31T15:13:26.390058Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "\n",
    "# Set the starting date\n",
    "this_date = datetime.datetime(2016,1,1)\n",
    "# Set the end date\n",
    "end_date = datetime.datetime(2016,1,5)\n",
    "# Increase the date by one day\n",
    "time_delta = datetime.timedelta(days=1)\n",
    "\n",
    "# Main loop\n",
    "while this_date <= end_date:\n",
    "    url = \"https://n5eil01u.ecs.nsidc.org/DP5/MOST/MOD10A1.006/{:s}/\".format(this_date.strftime(\"%Y.%m.%d\"))\n",
    "    url_downloader(username, password, url=url)\n",
    "    this_date = this_date + time_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise A Different Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "            \n",
    "We have now dowloaded a different dataset, the [MOD10A product](http://www.icess.ucsb.edu/modis/SnowUsrGuide/usrguide_1dtil.html), which is the 500 m MODIS daily snow cover product, over the UK.\n",
    "\n",
    "This is a good opportunity to see if you can apply what was learned above about interpreting QC information and using `gdal` to examine a dataset.\n",
    "\n",
    "If you examine the [data description page](http://nsidc.org/data/MOD10A1), you will see that the data are in HDF EOS format (the same as the LAI product). \n",
    "\n",
    "### Download\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Download the MODIS Terra daily snow product for the UK for the year 2012 for the month of February and put them in the directory `data`.\n",
    "</div>\n",
    "\n",
    "###  Explore\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Show all of the subset data layers in this dataset. \n",
    "</div>\n",
    "\n",
    "### Read a dataset\n",
    "\n",
    "Suppose we are interested in the dataset `NDSI_Snow_Cover` over the land surface.\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Read this dataset for one of the files into a numpy array and show a plot of the dataset.\n",
    "</div>\n",
    "\n",
    "### Water mask\n",
    "\n",
    "\n",
    "The [data description page](http://nsidc.org/data/MOD10A1) tells us that values of `239` will indicate whether the data is ocean. You can use this information to build the water mask.\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Demonstrate how to build a water mask from one of these files, setting the mask `False` for land and `True` for water. \n",
    "\n",
    "Produce a plot of this.\n",
    "</div>\n",
    "\n",
    "### Valid pixel mask\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "As well as having a land/water mask, we should generate a mask for valid pixels. For the snow dataset, values between 0 and 100 (inclusive) represent valid snow cover data values. Other values are not valid for some reason. Set the mask to `False` for valid pixels and `True` for others. Produce a plot of the mask. \n",
    "</div>\n",
    "\n",
    "### 3D dataset\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Generate a 3D masked numpy array using the valid pixel mask for masking, of `Fractional_Snow_Cover` for each day of February 2012. \n",
    "\n",
    "You might like to produce a movie of the result.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Vector masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will use a pre-defined function to generate a mask from some vector boundary data.\n",
    "\n",
    "In this case, we will generate a mask for Ireland, projected into the coordinate system of the  MODIS LAI dataset, and use that to generate a new LAI data only for Ireland."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, geospatial data is acquired and recorded for particular geometric objects such as polygons or lines. An example is a road layout, where each road is represented as a geometric object (a line, with points given in a geographical projection), with a number of added *features* associated with it, such as the road name, whether it is a toll road, or whether it is dual-carriageway, etc. This data is quite different to a raster, where the entire scene is tessellated into pixels, and each pixel holds a value (or an array of value in the case of multiband rasterfiles). \n",
    "\n",
    "If you are familiar with databases, vector files are effectively a database, where one of the fields is a geometry object (a line in our previous road example, or a polygon if you consider a cadastral system). We can thus select different records by writing queries on the features. Some of these queries might be spatial (e.g. check whether a point is inside a particular country polygon).\n",
    "\n",
    "The most common format for vector data is the **ESRI Shapfile**, which is a multifile format (i.e., several files are needed in order to access the data). We'll start by getting hold of a shapefile that contains the countries of the world as polygons, together with information on country name, capital name, population, etc. The file is available [here](http://www.naturalearthdata.com/features/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will download the file using `requests`, and we will use the Python [`zipfile`](https://pymotw.com/2/zipfile/) module to extract the contents of the zip file in the `./data/` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-31T15:13:53.898363Z",
     "start_time": "2017-10-31T15:13:52.188930Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "\n",
    "\n",
    "zip_file_name = \"data/ne_50m_admin_0_countries.zip\"\n",
    "# Download file\n",
    "r = requests.get (\"http://www.naturalearthdata.com/http//www.naturalearthdata.com/\" +\n",
    "                  \"download/10m/cultural/ne_50m_admin_0_countries.zip\")\n",
    "if r.ok:\n",
    "    with open ( zip_file_name, 'w') as fp:\n",
    "        fp.write(r.content)\n",
    "# Unzip file\n",
    "zip_ref = zipfile.ZipFile(zip_file_name, 'r')\n",
    "zip_ref.extractall(\"./data/\")\n",
    "zip_ref.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check on the UNIX shell that the zip file has been both downloaded, and its contents extracted. We can see that we have a bunch of new files with extensions like `.dbf, .prj, .shx` and `.shp`. The latter is the main file, the other files are auxiliary (but they are all needed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-31T15:13:54.047976Z",
     "start_time": "2017-10-31T15:13:53.904260Z"
    }
   },
   "outputs": [],
   "source": [
    "!ls -lh data/ne*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to import `ogr`, and then open the file. As with GDAL, we get a handler to the file, (`g` in this case). OGR files can have different layers, although Shapefiles only have one. We need to select the layer using `GetLayer(0)` (selecting the first layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-31T15:13:54.079070Z",
     "start_time": "2017-10-31T15:13:54.059616Z"
    }
   },
   "outputs": [],
   "source": [
    "from osgeo import ogr\n",
    "\n",
    "g = ogr.Open( \"data/ne_50m_admin_0_countries.shp\" )\n",
    "layer = g.GetLayer( 0 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see a field (the field `NAME`) we can loop over the features in the layer, and use the `GetField('NAME')` method. We'll only do ten features here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-31T15:13:54.121350Z",
     "start_time": "2017-10-31T15:13:54.086557Z"
    }
   },
   "outputs": [],
   "source": [
    "for n_feat, feat in enumerate(layer):\n",
    "    print \"Feature #{:d}: {:s}\".format(n_feat+1, feat.GetField('NAME'))\n",
    "    if n_feat == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wanted to see the different layers, we could do this using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-31T15:13:54.169720Z",
     "start_time": "2017-10-31T15:13:54.127777Z"
    }
   },
   "outputs": [],
   "source": [
    "layerDefinition = layer.GetLayerDefn()\n",
    "\n",
    "for i in range(layerDefinition.GetFieldCount()):\n",
    "    print \"Field %d: %s\" % ( i+1, layerDefinition.GetFieldDefn(i).GetName() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is much more information on using `ogr` on the associated [notebook OGR_Python](OGR_Python.ipynb) that you should explore at some point.\n",
    "\n",
    "One thing we may often wish to dowith such vector datsets is produce a mask, e.g. for national boundaries. One of the complexities of this is changing the projection that the vector data come in to that of the raster dataset.  \n",
    "\n",
    "This is too involved to go over in this session, so we will simply present you with a function to achieve this. The function is available in [`python/raster_mask.py`](./python/raster_mask.py), which we'll import into the system by first adding the `python` folder to the Python path. The function is called `rasterise_vector`. Let's see the help..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-31T15:13:54.209539Z",
     "start_time": "2017-10-31T15:13:54.174590Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,\"./python/\")\n",
    "\n",
    "from raster_mask import rasterise_vector\n",
    "help(rasterise_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, at the very least, we need to \n",
    "\n",
    "1. Give the raster set that we want to use as a reference. The mask will have the same properties as this raster (e.g. projection, extent, number of pixels, ...).\n",
    "2. Give a vector file (in this case, the world borders file).\n",
    "3. Select one country using the slightly awkward SQL notation of `\"field_name='Ireland'\"` (e.g.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-31T15:16:21.004763Z",
     "start_time": "2017-10-31T15:16:20.667527Z"
    }
   },
   "outputs": [],
   "source": [
    "filename = 'data/MCD15A2.A2012273.h17v03.005.2012297134400.hdf'\n",
    "\n",
    "# a layer (doesn't matter so much which: use for geometry info)\n",
    "layer = 'Lai_1km'\n",
    "# the full dataset specification\n",
    "file_template = 'HDF4_EOS:EOS_GRID:\"%s\":MOD_Grid_MOD15A2:%s'\n",
    "file_spec = file_template % ( filename, layer)\n",
    "\n",
    "M = rasterise_vector ( file_spec, \"data/ne_50m_admin_0_countries.shp\", \n",
    "                  \"NAME='Ireland'\", verbose=False)\n",
    "plt.imshow(M, interpolation=\"nearest\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is available as [python/raster_mask.py](python/raster_mask.py).\n",
    "\n",
    "Most of the code below should be familiar from above (we make use of the `getLAI()` function we developed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-31T15:17:56.474248Z",
     "start_time": "2017-10-31T15:17:55.868676Z"
    }
   },
   "outputs": [],
   "source": [
    "from raster_mask import getLAI\n",
    "\n",
    "\n",
    "# the data file name\n",
    "filename = 'data/MCD15A2.A2012273.h17v03.005.2012297134400.hdf'\n",
    "\n",
    "# a layer (doesn't matter so much which: use for geometry info)\n",
    "layer = 'Lai_1km'\n",
    "# the full dataset specification\n",
    "file_template = 'HDF4_EOS:EOS_GRID:\"%s\":MOD_Grid_MOD15A2:%s'\n",
    "file_spec = file_template%(filename,layer)\n",
    "\n",
    "mask = rasterise_vector ( file_spec, \"data/ne_50m_admin_0_countries.shp\", \n",
    "                  \"NAME='Ireland'\", verbose=False)\n",
    "plt.imshow(mask)\n",
    "# get the LAI data\n",
    "data = getLAI(filename)\n",
    "\n",
    "# reset the data mask\n",
    "# 'mask' is True for Ireland\n",
    "# so take the opposite \n",
    "data['Lai_1km'] = ma.array(data['Lai_1km'], mask=np.logical_not(mask))\n",
    "data['LaiStdDev_1km'] = ma.array(data['Lai_1km'], mask=np.logical_not(mask))\n",
    "\n",
    "plt.title('LAI for Eire: 2012273')\n",
    "plt.imshow(data['Lai_1km'],vmax=6)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the concepts above to generate a 3D masked numpy data array of LAI and std LAI for Eire for the year 2012.\n",
    "\n",
    "Plot your results and make a move of LAI.\n",
    "\n",
    "Plot average LAI for Eire as a function of day of year for 2012."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session, we have learned to use some geospatial tools using GDAL in Python. A good set of [working notes on how to use GDAL](http://jgomezdans.github.io/gdal_notes/) has been developed that you will find useful for further reading, as well as looking at the [advanced](advanced.ipynb) section.\n",
    "\n",
    "We have also very briefly introduced dealing with vector datasets in `ogr`, but this was mainly through the use of a pre-defined function that will take an ESRI shapefile (vector dataset), warp this to the projection of a raster dataset, and produce a mask for a given layer in the vector file.\n",
    "\n",
    "If there is time in the class, we will develop some exercises to examine the datasets we have generated and/or to explore some different datasets or different locations.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
