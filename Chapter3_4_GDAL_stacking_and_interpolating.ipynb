{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 Stacking and interpolating data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#3.4-Stacking-and-interpolating-data\" data-toc-modified-id=\"3.4-Stacking-and-interpolating-data-1\">3.4 Stacking and interpolating data</a></span><ul class=\"toc-item\"><li><span><a href=\"#3.4.1-Introduction\" data-toc-modified-id=\"3.4.1-Introduction-1.1\">3.4.1 Introduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#3.4.1.1-Test-your-login\" data-toc-modified-id=\"3.4.1.1-Test-your-login-1.1.1\">3.4.1.1 Test your login</a></span></li><li><span><a href=\"#3.4.1.2-Get-the-datasets-for-today\" data-toc-modified-id=\"3.4.1.2-Get-the-datasets-for-today-1.1.2\">3.4.1.2 Get the datasets for today</a></span></li></ul></li><li><span><a href=\"#3.4.2-QA-data\" data-toc-modified-id=\"3.4.2-QA-data-1.2\">3.4.2 QA data</a></span></li><li><span><a href=\"#3.4.2-A-time-series\" data-toc-modified-id=\"3.4.2-A-time-series-1.3\">3.4.2 A time series</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[up to 3.0](Chapter3_1_GDAL.ipynb)]\n",
    "\n",
    "\n",
    "## 3.4.1 Introduction\n",
    "\n",
    "In this section, we will:\n",
    "\n",
    "* develop code to produce a stacked dataset of spatio-temporal data on a grid\n",
    "* interpolate over any missing data\n",
    "* smooth the dataset\n",
    "\n",
    "### 3.4.1.1 Test your login\n",
    "\n",
    "Let's first test your NASA login:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geog0111.nasa_requests as nasa_requests\n",
    "from geog0111.cylog import cylog\n",
    "%matplotlib inline\n",
    "\n",
    "url = 'https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2018.09.30/' \n",
    "        \n",
    "# grab the HTML information\n",
    "try:\n",
    "    html = nasa_requests.get(url).text\n",
    "    # test a few lines of the html\n",
    "    if html[:20] == '<!DOCTYPE HTML PUBLI':\n",
    "        print('this seems to be ok ... ')\n",
    "        print('use cylog().login() anywhere you need to specify the tuple (username,password)')\n",
    "except:\n",
    "    print('login error ... try entering your username password again')\n",
    "    print('then re-run this cell until it works')\n",
    "    cylog(init=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1.2 Get the datasets for today\n",
    "\n",
    "Now let's get the datasets we need for today. These files have been pre-downloaded on the UCL Geography system. They are available through:\n",
    "\n",
    "[http://www2.geog.ucl.ac.uk/~plewis/geog0111_data](http://www2.geog.ucl.ac.uk/~plewis/geog0111_data) or [http://www2.geog.ucl.ac.uk/~ucfajlg/geog0111_data](http://www2.geog.ucl.ac.uk/~ucfajlg/geog0111_data)\n",
    "\n",
    "but the filenames can be locally installed using [`procure_dataset()`](geog0111/geog_data.py) as demonstrated in the code block below.\n",
    "\n",
    "**You should run this section before the class starts to save time.**\n",
    "\n",
    "It should be quite rapid if you are on the UCL Geography system.\n",
    "\n",
    "On a home network, it is likely to trake more than an hour as it has to fully download the files.\n",
    "\n",
    "You are given the relevant filenames (for 2016 and 2017 `MCD15A3H` data for the tiles `h1[7-8]v0[3-4]`) in the files [`data/lai_filelist_{year}.dat.txt`](data/lai_filelist_2016.dat.txt). The datasets have been pre-downloaded for this exercise, but you need to copy then to the local filespace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from geog0111.geog_data import *\n",
    "\n",
    "destination_folder = Path('data')\n",
    "if not destination_folder.exists():\n",
    "        dest_path.mkdir()\n",
    "\n",
    "# we have the filenames provided \n",
    "# in data/lai_filelist_2016.dat.txt\n",
    "for year in [2016,2017]:\n",
    "    control_file = f'data/lai_filelist_{year}.dat.txt'\n",
    "    # read the ascii data from the file in\n",
    "    filenames = open(control_file).read().split()\n",
    "\n",
    "    # get the local files\n",
    "    # set verbose=True if you want to see what is happening\n",
    "    done = [procure_dataset(f,\\\n",
    "                verbose=False,\\\n",
    "                destination_folder=destination_folder) \n",
    "                                    for f in filenames]\n",
    "    # done should be all True if this has worked\n",
    "\n",
    "    # print the first 8 in the list, just to see it looks ok\n",
    "    print(f'\\n {year}\\n','*'*len(str(year)))\n",
    "    for f in filenames[:8]:\n",
    "        print (f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now make sure you have the world borders ESRI shape file you need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import shutil \n",
    "from pathlib import Path\n",
    "\n",
    "# zip file\n",
    "zipfile = 'TM_WORLD_BORDERS-0.3.zip'\n",
    "# URL\n",
    "tm_borders_url = f\"http://thematicmapping.org/downloads/{zipfile}\"\n",
    "# destibnation folder\n",
    "destination_folder = Path('data')\n",
    "\n",
    "# set up some filenames\n",
    "zip_file = destination_folder.joinpath(zipfile)\n",
    "shape_file = zip_file.with_name(zipfile.replace('zip','shp'))\n",
    "\n",
    "# download zip if need to\n",
    "if not Path(zip_file).exists():\n",
    "    r = requests.get(tm_borders_url)\n",
    "    with open(zip_file, 'wb') as fp:\n",
    "        fp.write (r.content)\n",
    "\n",
    "# extract shp from zip if need to\n",
    "if not Path(shape_file).exists():\n",
    "    shutil.unpack_archive(zip_file.as_posix(),\n",
    "                         extract_dir=destination_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.2 QA data\n",
    "\n",
    "need to look at QA data for dataset (and/or uncertaiunty).\n",
    "\n",
    "For example, if interpolating data, we would want to base the weight we put on any sample on the 'quality' of that sample. This will be expressed by either some QA assessment ('good', 'ok', 'bad') or some measure of uncertainty (or both).\n",
    "\n",
    "Here, we will use the QA information in the LAI product to generate a sample weighting scheme. We shall later use this weighting for data smoothing and interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdal\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from geog0111.create_blank_file import create_blank_file\n",
    "from datetime import datetime\n",
    "\n",
    "destination_folder = Path('data')\n",
    "year = 2017\n",
    "product = 'MCD15A3H'\n",
    "version = 6\n",
    "tile = 'h1[7-8]v0[3-4]'\n",
    "# Luxembourg\n",
    "FIPS = \"LU\"\n",
    "\n",
    "tile_ = tile.replace('[','_').replace(']','_').replace('-','')+FIPS\n",
    "\n",
    "shape_file = destination_folder.\\\n",
    "                 joinpath('TM_WORLD_BORDERS-0.3.shp').as_posix()\n",
    "\n",
    "doy = 149\n",
    "\n",
    "ipfile = destination_folder.\\\n",
    "                joinpath(f'{product}.A{year}{doy:03d}.{tile_}.{version:03d}').as_posix()\n",
    "\n",
    "opfile = ipfile.replace(f'{doy:03d}.','').replace(tile,tile_)\n",
    "\n",
    "filenames = list(destination_folder\\\n",
    "                .glob(f'{product}.A{year}{doy:03d}.{tile}.{version:03d}.*.hdf'))\n",
    "\n",
    "ofiles = []\n",
    "params =  ['Lai_500m', 'FparLai_QC']\n",
    "for d in params:\n",
    "    dataset_names = sorted([f'HDF4_EOS:EOS_GRID:'+\\\n",
    "                         f'\"{file_name.as_posix()}\":'+\\\n",
    "                         f'MOD_Grid_MCD15A3H:{d}'\\\n",
    "                            for file_name in filenames])\n",
    "\n",
    "    spatial_file = f'{opfile}.{doy:03d}.{d}.vrt'\n",
    "    clipped_file = f'{opfile}.{doy:03d}_clip.{d}.vrt'\n",
    "    g = gdal.BuildVRT(spatial_file, dataset_names)\n",
    "    if(g):\n",
    "        del(g)\n",
    "        g = gdal.Warp(clipped_file,\\\n",
    "                                   spatial_file,\\\n",
    "                                   format='VRT', dstNodata=255,\\\n",
    "                                   cutlineDSName=shape_file,\\\n",
    "                                   cutlineWhere=f\"FIPS='{FIPS}'\",\\\n",
    "                                   cropToCutline=True)\n",
    "        if (g):\n",
    "            del(g)\n",
    "        ofiles.append(clipped_file)\n",
    "print(ofiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "lai = [gdal.Open(ofiles[i]).ReadAsArray() for i in range(len(params))]\n",
    "\n",
    "lai[0] = lai[0] * 0.1\n",
    "# if we want bit field 5-7\n",
    "# we form a binary mask\n",
    "mask57 = 0b11100000\n",
    "# and right shift 5 (>> 5)\n",
    "lai[1] = (lai[1] & mask57) >> 5\n",
    "# 0 to 3 are good\n",
    "scale = 0.61803398875\n",
    "lai[1] = (scale**0) * (lai[1] == 0).astype(float) + \\\n",
    "         (scale**1) * (lai[1] == 1).astype(float) + \\\n",
    "         (scale**2) * (lai[1] == 2).astype(float) + \\\n",
    "         (scale**3) * (lai[1] == 3).astype(float)\n",
    "    \n",
    "\n",
    "    \n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, sharex=True, sharey=True,\n",
    "                       figsize=(10,5))\n",
    "axs = np.array(axs).T.flatten()\n",
    "\n",
    "for i in range(len(params)):\n",
    "    img = axs[i].imshow(lai[i], interpolation=\"nearest\",\n",
    "                 cmap=plt.cm.inferno_r)\n",
    "    axs[i].set_title(params[i])\n",
    "    plt.colorbar(img,ax=axs[i],shrink=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.2 A time series\n",
    "\n",
    "You should now know how to access and download datasets from the NASA servers and have developed functions to do this.\n",
    "\n",
    "You should also know how to select a dataset from a set of hdf files, and mosaic, mask and crop the data to correspond to some vector boundary. This is a very common task in geospatial processing.\n",
    "\n",
    "We now consider the case where we want to analyse a time series of data. We will use LAI over time to exemplify this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdal\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from geog0111.create_blank_file import create_blank_file\n",
    "from datetime import datetime\n",
    "\n",
    "destination_folder = Path('data')\n",
    "year = 2017\n",
    "product = 'MCD15A3H'\n",
    "version = 6\n",
    "tile = 'h1[7-8]v0[3-4]'\n",
    "params =  ['Lai_500m', 'FparLai_QC']\n",
    "\n",
    "\n",
    "\n",
    "tile_ = tile.replace('[','_').replace(']','_').replace('-','')+FIPS\n",
    "\n",
    "shape_file = destination_folder.\\\n",
    "                 joinpath('TM_WORLD_BORDERS-0.3.shp').as_posix()\n",
    "\n",
    "allopfile = destination_folder.\\\n",
    "                joinpath(f'{product}.A{year}.{tile_}.{version:03d}')\n",
    "\n",
    "ndays_in_year = (datetime(year,12,31) - datetime(year,1,1)).days + 1\n",
    "\n",
    "\n",
    "for d in params:\n",
    "    old_clip = None\n",
    "    allvrt = []\n",
    "    bandNames = []\n",
    "    for doy in range(1,ndays_in_year+1,1):\n",
    "\n",
    "        ipfile = destination_folder.\\\n",
    "                    joinpath(f'{product}.A{year}{doy:03d}.{tile_}.{version:03d}').as_posix()\n",
    "\n",
    "        opfile = ipfile.replace(f'{doy:03d}.','').replace(tile,tile_)\n",
    "\n",
    "        filenames = destination_folder\\\n",
    "                    .glob(f'{product}.A{year}{doy:03d}.{tile}.{version:03d}.*.hdf')\n",
    "\n",
    "        dataset_names = sorted([f'HDF4_EOS:EOS_GRID:'+\\\n",
    "                             f'\"{file_name.as_posix()}\":'+\\\n",
    "                             f'MOD_Grid_MCD15A3H:{d}'\\\n",
    "                                for file_name in filenames])\n",
    "        spatial_file = f'{opfile}.{doy:03d}.{d}.vrt'\n",
    "        clipped_file = f'{opfile}.{doy:03d}_clip.{d}.vrt'\n",
    "        if len(dataset_names):\n",
    "            g = gdal.BuildVRT(spatial_file, dataset_names)\n",
    "            if(g):\n",
    "                del(g)\n",
    "                g = gdal.Warp(clipped_file,\\\n",
    "                                   spatial_file,\\\n",
    "                                   format='VRT', dstNodata=255,\\\n",
    "                                   cutlineDSName=shape_file,\\\n",
    "                                   cutlineWhere=f\"FIPS='{FIPS}'\",\\\n",
    "                                   cropToCutline=True)\n",
    "        elif old_clip:\n",
    "            blank_file_tiff = f'{opfile}_blank.tiff'\n",
    "            # generate a blank dataset in case of missing days\n",
    "            if not Path(blank_file_tiff).exists():\n",
    "                # copy info\n",
    "                create_blank_file(old_clip,blank_file_tiff,value=255)\n",
    "\n",
    "            # build a vrt\n",
    "            g = gdal.BuildVRT(clipped_file, [blank_file_tiff])\n",
    "\n",
    "        if (g):\n",
    "            del(g)\n",
    "            bandNames.append(f'DOY {doy:03d}')\n",
    "            allvrt.append(clipped_file)\n",
    "\n",
    "        old_clip = clipped_file\n",
    "\n",
    "\n",
    "\n",
    "    g = gdal.BuildVRT(f'{allopfile.as_posix()}.{d}.vrt', allvrt,\\\n",
    "                      options=gdal.BuildVRTOptions(VRTNodata=255,\\\n",
    "                                                   srcNodata=255,\\\n",
    "                                                   allowProjectionDifference=True,\\\n",
    "                                                   separate=True))\n",
    "    if (g):\n",
    "        # set band names\n",
    "        for i in range(g.RasterCount):\n",
    "            g.GetRasterBand(i+1).SetDescription(bandNames[i])\n",
    "\n",
    "        # close and flush file\n",
    "        del g\n",
    "        print (f'{allopfile.as_posix()}.{d}.vrt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdal\n",
    "import numpy as np\n",
    "\n",
    "destination_folder = Path('data')\n",
    "year = 2017\n",
    "product = 'MCD15A3H'\n",
    "version = 6\n",
    "tile = 'h1[7-8]v0[3-4]'\n",
    "params =  ['Lai_500m', 'FparLai_QC']\n",
    "\n",
    "allopfile = destination_folder.\\\n",
    "                joinpath(f'{product}.A{year}.{tile_}.{version:03d}')\n",
    "lai = []\n",
    "for d in params:\n",
    "    \n",
    "    g = gdal.Open(f'{allopfile.as_posix()}.{d}.vrt',gdal.GA_ReadOnly)\n",
    "    data = np.array([g.GetRasterBand(b+1).ReadAsArray() \\\n",
    "                for b in range(g.RasterCount)])\n",
    "\n",
    "    lai.append(data)\n",
    "\n",
    "lai[0] = lai[0] * 0.1\n",
    "# if we want bit field 5-7\n",
    "# we form a binary mask\n",
    "mask57 = 0b11100000\n",
    "# and right shift 5 (>> 5)\n",
    "lai[1] = (lai[1] & mask57) >> 5\n",
    "# 0 to 3 are good\n",
    "scale = 0.61803398875\n",
    "lai[1] = (scale**0) * (lai[1] == 0).astype(float) + \\\n",
    "         (scale**1) * (lai[1] == 1).astype(float) + \\\n",
    "         (scale**2) * (lai[1] == 2).astype(float) + \\\n",
    "         (scale**3) * (lai[1] == 3).astype(float)\n",
    "    \n",
    "print(lai[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate\n",
    "import scipy.ndimage.filters\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "weight = lai[1]\n",
    "\n",
    "# filter, in units of days\n",
    "sigma = 8\n",
    "gx = np.arange(-3*sigma, 3*sigma, 1)\n",
    "gaussian = np.exp(-(gx/sigma)**2/2.)\n",
    "plt.plot(gx,gaussian)\n",
    "x = scipy.ndimage.filters.convolve1d(lai[0] * weight, gaussian, axis=0,mode='wrap')\n",
    "w = scipy.ndimage.filters.convolve1d(weight, gaussian, axis=0,mode='wrap')\n",
    "ilai = x/w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## find where the weight is highest, and lets look there!\n",
    "sweight = weight.sum(axis=0)\n",
    "r,c = np.where(sweight == np.max(sweight))\n",
    "plt.title(f'{product} {FIPS} {params[0]} {year} {r[0]},{c[0]}')\n",
    "plt.plot((ilai)[:,r[0],c[0]],'r--')\n",
    "plt.plot((lai[0])[:,r[0],c[0]],'+')\n",
    "plt.ylim(0,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "making movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import tempfile\n",
    "\n",
    "# lai movie as animated gif\n",
    "\n",
    "destination_folder = Path('images')\n",
    "year = 2017\n",
    "product = 'MCD15A3H'\n",
    "version = 6\n",
    "tile = 'h1[7-8]v0[3-4]'\n",
    "params =  ['Lai_500m', 'FparLai_QC']\n",
    "\n",
    "tile_ = tile.replace('[','_').replace(']','_').replace('-','')+FIPS\n",
    "allopfile = destination_folder.\\\n",
    "                joinpath(f'{product}.A{year}.{tile_}.{version:03d}')\n",
    "\n",
    "images = []\n",
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    ofile = f'{tmpdirname}/tmp.png'\n",
    "    \n",
    "    for i in range(ilai.shape[0]):\n",
    "        plt.figure(0,figsize=(10,6))\n",
    "        plt.clf()\n",
    "        plt.imshow(ilai[i],vmin=0,vmax=6,cmap=plt.cm.inferno_r)\n",
    "        plt.title(f'{product} {FIPS} {params[0]} {year} DOY {i+1:03d}')\n",
    "        plt.colorbar(shrink=0.85)\n",
    "        plt.savefig(ofile)    \n",
    "        images.append(imageio.imread(ofile))\n",
    "imageio.mimsave(f'{allopfile}.gif', images)\n",
    "print(f'{allopfile}.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/MCD15A3H.A2017.h1_78_v0_34_LU.006.gif)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "362.390625px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
