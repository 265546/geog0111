{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Accessing MODIS Data products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.1 Introduction\n",
    "\n",
    "[[up to 3.1](Chapter3_GDAL.ipynb)]\n",
    "\n",
    "In this section, you will learn how to:\n",
    "\n",
    "* scan the directories (on the Earthdata server) where the MODIS data are stored\n",
    "* get the dataset filename for a given tile, date and product\n",
    "* get to URL associated with the dataset\n",
    "* use the URL to pull the dataset over to store in the local file system\n",
    "\n",
    "You should already know:\n",
    "\n",
    "* basic use of Python (sections 1 and 2)\n",
    "* the MODIS product grid system\n",
    "* the two tiles needed to cover the UK \n",
    "\n",
    "        tiles = ['h17v03', 'h18v03']\n",
    "        \n",
    "* what LAI is and the code for the MODIS LAI/FPAR product [MOD15](https://modis.gsfc.nasa.gov/data/dataprod/mod15.php)\n",
    "* your username and password for [NASA Earthdata](https://urs.earthdata.nasa.gov/home), or have previously entered this with [`cylog`](geog0111/cylog.py).\n",
    "\n",
    "Let's first just test your NASA login:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this seems to be ok ... \n",
      "use cylog().login() anywhere you need to specify the tuple (username,password)\n"
     ]
    }
   ],
   "source": [
    "import geog0111.nasa_requests as nasa_requests\n",
    "\n",
    "url = 'https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2018.09.30/' \n",
    "        \n",
    "# grab the HTML information\n",
    "html = nasa_requests.get(url).text\n",
    "\n",
    "# test a few lines of the html\n",
    "if html[:20] == '<!DOCTYPE HTML PUBLI':\n",
    "    print('this seems to be ok ... ')\n",
    "    print('use cylog().login() anywhere you need to specify the tuple (username,password)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the local class `geog0111.nrequests` here, in place of the usual `requests` as this lets the user avoid exposure to some of the tricky bits of getting data from the NASA server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3.2.2  Accessing NASA MODIS URLs\n",
    "\n",
    "Although you can access MODIS datasets through the [NASA Earthdata](https://urs.earthdata.nasa.gov/home) interface, there are many occasions that we would want to just automatically pull datasets. This is particularly true when you want a time series of data that might involve many files. For example, for analysing LAI or other variables over space/time) we will want to write code that pulls the time series of data. \n",
    "\n",
    "This is also something you will need to do the your assessed practical.\n",
    "\n",
    "If the data we want to use are accessible to us as a URL, we can simply use `requests` as in previous exercises.\n",
    "\n",
    "Sometimes, we will be able to specify the parameters of the dataset we want, e.g. using [JSON](https://www.json.org). At othertimes (as in the case here) we might need to do a little work ourselves to construct the particular URL we want.\n",
    "\n",
    "If you visit the site [https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006](https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006), you will see 'date' style links (e.g. `2018.09.30`) through to sub-directories. \n",
    "\n",
    "In these, e.g. [https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2018.09.30/](https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2018.09.30/) you will find URLs of a set of files. \n",
    "\n",
    "The files pointed to by the URLs are the MODIS MOD15 4-day composite 500 m LAI/FPAR product [MCD15A3H](https://lpdaac.usgs.gov/dataset_discovery/modis/modis_products_table/mcd15a3h_v006).\n",
    "\n",
    "There are links to several datasets on the page, including 'quicklook files' that are jpeg format images of the datasets, e.g.:\n",
    "\n",
    "![MCD15A3H.A2018273.h17v03](https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2018.09.30/BROWSE.MCD15A3H.A2018273.h17v03.006.2018278143630.1.jpg)\n",
    "\n",
    "as well as `xml` files and `hdf` datasets. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2.1 `datetime`\n",
    "\n",
    "The URL we have used above, [https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2018.09.30/](https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2018.09.30/) starts with a call to the server directory `MOTA`, so we can think of `https://e4ftl01.cr.usgs.gov/MOTA` asd the base level URL.\n",
    "\n",
    "The rest of the directoy information `MCD15A3H.006/2018.09.30` tells us:\n",
    "\n",
    "* the product name `MCD15A3H`\n",
    "* the product version `006`\n",
    "* the date of the dataset `2018.09.30`\n",
    "\n",
    "There are several ways we could specify the date information. The most 'human readable' is probably `YYYY.MM.DD` as given here. \n",
    "\n",
    "Sometimes we will want to refer to it by 'day of year' (`doy`) (sometimes mistakenly referred to as [Julian day](https://en.wikipedia.org/wiki/Julian_day)) for a particular year. Day of year will be an integer that goes from 1 to 365 or 366 (inclusive).\n",
    "\n",
    "We can use the Python `datetime` to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "year = 2018\n",
    "\n",
    "for doy in [1,60,365,366]:\n",
    "    # set it up as Jan 1st, plus doy - 1\n",
    "    d = datetime.datetime(year,1,1) + datetime.timedelta(doy-1)\n",
    "    \n",
    "    # note the careful formatting to include zeros in datestr\n",
    "    datestr = f'{d.year:4d}.{d.month:02d}.{d.day:02d}'\n",
    "\n",
    "    print(f'doy {doy:3d} in {year} is {datestr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.2.1**\n",
    "\n",
    "* copy the above code, and change the year to a leap year to see if it works as expected\n",
    "* write some code that loops over each day in the year and converts from `doy` to the format of `datestr` above.\n",
    "* modify the code so that it forms the full directory URL for the MODIS dataset, e.g. ` https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2018.09.30/` for each `doy`\n",
    "* use what you have learned to write a function called `get_url()`, which you give the year and day of year and which returns the full URL. It should use keywords to define `product`, `version` and `base_url`. \n",
    "* For homework, tidy up your function, making sure you document it properly. Think aboiut what might happen if you enter incorrect information.\n",
    "\n",
    "**Hint**: \n",
    "    \n",
    "1. number of days in year\n",
    "    \n",
    "    ndays_in_year = (datetime.datetime(year,12,31) - datetime.datetime(year,1,1)).days + 1\n",
    "    \n",
    "Remember that `doy` goes from 1 to 365 or 366 (inclusive).\n",
    "\n",
    "2. `datestr` format\n",
    "\n",
    "We use `datestr = f'{d.year:4d}.{d.month:02d}.{d.day:02d}'` as the date string format. The elements such as `{d.year:4d}` mean that `d.year` is interpreted as an integer (`d`) of length `4`. When we put a `0` in front, such as in `02d` the resultant string is 'padded' with `0`. Try something like:\n",
    "\n",
    "    value = 10\n",
    "    print(f'{value:X10f}')\n",
    "\n",
    "3. some bigger hints ...\n",
    "\n",
    "To get the full URL, you will probably want to define something along the lines of:\n",
    "    \n",
    "    url = f'{base_url}/{product}.{version:03d}/{datestr}'\n",
    "    \n",
    "assuming version is an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do exercise here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2.2 html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we access this 'listing' (directory links such as [https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2018.09.30/](https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2018.09.30/)) from Python, we will obtain the information in [HTML](https://www.w3schools.com/html/). We don't expect you to know this language in any great depth, but knowing some of the basics is oftem useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2018.09.30\n",
      "<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\">\n",
      "<html>\n",
      " <head>\n",
      "  <title>Index of /MOTA/MCD15A3H.006/2018.09.30</title>\n",
      " </head>\n",
      " <body>\n",
      "<pre>\n",
      "********************************************************************************\n",
      "\n",
      "                         U.S. GOVERNMENT COMPUTER\n",
      "\n",
      "This US Government computer is for authorized users only.  By accessing this\n",
      "system you are consenting to complete monitoring with no expectation of privacy.\n",
      "Unauthorized access or use may subject you to disciplinary action and criminal\n",
      "prosecution.\n",
      "\n",
      "Attention user: You are downloading data from NASA's Land Processes Distributed\n",
      "Active Archive Center (LP DAAC) located at the USGS Earth Resources Observation and\n",
      "Science (EROS) Center.\n",
      "\n",
      "Downloading these data requires a NASA Earthdata Login username and password.\n",
      "To obtain a NASA Earthdata Login account, please visit\n",
      "<a href=\"https://urs.earthdata.nasa.gov/users/new\">https://urs.earthdata.nasa.gov/users/new/</a>\n",
      "\n",
      " ------------------------------ etc ------------------------------\n",
      "\n",
      "<img src=\"/icons/unknown.gif\" alt=\"[   ]\"> <a href=\"MCD15A3H.A2018273.h35v08.006.2018278143649.hdf.xml\">MCD15A3H.A2018273.h35v08.006.2018278143649.hdf.xml</a>      2018-10-05 09:42  7.6K  \n",
      "<img src=\"/icons/unknown.gif\" alt=\"[   ]\"> <a href=\"MCD15A3H.A2018273.h35v09.006.2018278143649.hdf\">MCD15A3H.A2018273.h35v09.006.2018278143649.hdf</a>          2018-10-05 09:42  207K  \n",
      "<img src=\"/icons/unknown.gif\" alt=\"[   ]\"> <a href=\"MCD15A3H.A2018273.h35v09.006.2018278143649.hdf.xml\">MCD15A3H.A2018273.h35v09.006.2018278143649.hdf.xml</a>      2018-10-05 09:42  7.6K  \n",
      "<img src=\"/icons/unknown.gif\" alt=\"[   ]\"> <a href=\"MCD15A3H.A2018273.h35v10.006.2018278143650.hdf\">MCD15A3H.A2018273.h35v10.006.2018278143650.hdf</a>          2018-10-05 09:42  298K  \n",
      "<img src=\"/icons/unknown.gif\" alt=\"[   ]\"> <a href=\"MCD15A3H.A2018273.h35v10.006.2018278143650.hdf.xml\">MCD15A3H.A2018273.h35v10.006.2018278143650.hdf.xml</a>      2018-10-05 09:42  7.6K  \n",
      "<hr></pre>\n",
      "</body></html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import geog0111.nasa_requests as nasa_requests\n",
    "from geog0111.get_url import get_url\n",
    "import datetime\n",
    "\n",
    "doy,year = 273,2018\n",
    "# use your get_url function\n",
    "# or the one supplied in geog0111\n",
    "url = get_url(doy,year).url\n",
    "print(url)\n",
    "\n",
    "# pull the html\n",
    "html = nasa_requests.get(url).text\n",
    "\n",
    "# print a few lines of the html\n",
    "print(html[:951])\n",
    "# etc\n",
    "print('\\n','-'*30,'etc','-'*30)\n",
    "# at the end\n",
    "print(html[-964:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In HTML the code text such as: \n",
    "\n",
    "    <a href=\"MCD15A3H.A2018273.h35v10.006.2018278143650.hdf\">MCD15A3H.A2018273.h35v10.006.2018278143650.hdf</a>  \n",
    "\n",
    "\n",
    "specifies an HTML link, that will appear as \n",
    "\n",
    "    MCD15A3H.A2018273.h35v10.006.2018278143650.hdf 2018-10-05 09:42  7.6K \n",
    "    \n",
    "and link to the URL specified in the `href` field: `MCD15A3H.A2018273.h35v10.006.2018278143650.hdf`.\n",
    "\n",
    "We could interpret this information by searching for strings etc., but the package `BeautifulSoup` can help us a lot in this.\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geog0111.nasa_requests as nasa_requests\n",
    "from geog0111.get_url import get_url\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "doy,year = 273,2018\n",
    "url = get_url(doy,year).url\n",
    "html = nasa_requests.get(url).text\n",
    "\n",
    "# use BeautifulSoup\n",
    "# to get all urls referenced with\n",
    "# html code <a href=\"some_url\">\n",
    "soup = BeautifulSoup(html,'lxml')\n",
    "links = [mylink.attrs['href'] for mylink in soup.find_all('a')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise E3.2.2**\n",
    "\n",
    "* copy the code in the block above and print out some of the linformation in the list `links` (e.g. the last 20 entries)\n",
    "* using an implicit loop, make a list called `hdf_filenames` of only those filenames (links) that have `hdf` as their filename extension.\n",
    "\n",
    "**Hint 1**: first you might select an example item from the `links` list: \n",
    "\n",
    "    item = links[-1]\n",
    "    print('item is',item)\n",
    "    \n",
    "and print:\n",
    "\n",
    "    item[-3:]\n",
    "        \n",
    "but maybe better (why would this be?) is:\n",
    "\n",
    "    item.split('.')[-1]\n",
    "    \n",
    "**Hint 2**: An implicit loop is a construct of the form:\n",
    "\n",
    "    [item for item in links]\n",
    "\n",
    "In an implicit for loop, we can actually add a conditional statement if we like, e.g. try:\n",
    "\n",
    "    hdf_filenames = [item for item in links if item[-5] == '4']\n",
    "    \n",
    "This will print out `item` if the condition `item[-5] == '4'` is met. That's a bit of a pointless test, but illustrates the pattern required. Try this now with the condition you want to use to select `hdf` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do exercise here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.3 MODIS filename format\n",
    "\n",
    "The `hdf` filenames are of the form:\n",
    "\n",
    "    MCD15A3H.A2018273.h35v10.006.2018278143650.hdf\n",
    "    \n",
    "where:\n",
    "\n",
    "* the first field (`MCD15A3H`) gives the product code\n",
    "* the second (`A2018273`) gives the observation date: day of year `273`, `2018` here\n",
    "* the third (`h35v10`) gives the 'MODIS tile' code for the data location\n",
    "* the remaining fields specify the product version number (`006`) and a code representing the processing date.\n",
    "\n",
    "If we want a particular dataset, we would assume then that we know the information to construct the first four fields.\n",
    "\n",
    "We then have the task remaining of finding an address of the pattern:\n",
    "\n",
    "    MCD15A3H.A2018273.h17v03.006.*.hdf\n",
    "    \n",
    "where `*` represents a wildcard (unknown element of the URL/filename).\n",
    "\n",
    "Putting together the code from above to get a list of the `hdf` files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from geog0111.nasa_requests import nasa_requests\n",
    "from bs4 import BeautifulSoup\n",
    "from geog0111.get_url import get_url\n",
    "import geog0111.nasa_requests as nasa_requests\n",
    "\n",
    "doy,year = 273,2018\n",
    "url = get_url(doy,year).url\n",
    "html = nasa_requests.get(url).text\n",
    "soup = BeautifulSoup(html,'lxml')\n",
    "links = [mylink.attrs['href'] for mylink in soup.find_all('a')]\n",
    "\n",
    "# get all files that end 'hdf' as in example above\n",
    "hdf_filenames = [item for item in links if item.split('.')[-1] == 'hdf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to specify a particular tile or tiles to access.\n",
    "\n",
    "In this case, we want to look at the field `item.split('.')[-4]` and check to see if it is the list `tiles`.\n",
    "\n",
    "**Exercise 3.2.3**\n",
    "\n",
    "* copy the code above and print out the first 10 values in the list `hdf_filenames`. Can you recognise where the tile information is in the string?\n",
    "\n",
    "Now, let's check what we get when we look at `item.split('.')[-4]`.\n",
    "\n",
    "* set a variable called `tiles` containing the names of the UK tiles (as in Exercise 3.1.1)\n",
    "* write a loop `for item in links:` to loop over each item in the list `links`\n",
    "* inside this loop set the condition `if item.split('.')[-1] == 'hdf':` to select only `hdf` files, as above\n",
    "* inside this conditional statement, print out `item.split('.')[-4]` to see if it looks like the tile names\n",
    "* having confirmed that you are getting the right information, add another conditional statement to see if `item.split('.')[-4] in tiles`, and then print only those filenames that pass both of your tests\n",
    "* see if you can combine the two tests (the two `if` statements) into a single one\n",
    "\n",
    "**Hint 1**: if you print all of the tilenames, this will go on for quite some time. Instead it may be better to use `print(item.split('.')[-4],end=' ')`, which will put a space, rather than a newline between each item printed.\n",
    "\n",
    "**Hint 2**: recall what the logical statement `(A and B)` gives when thinking about the combined `if` statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do exercise here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should end up with something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geog0111.nasa_requests as nasa_requests\n",
    "from bs4 import BeautifulSoup\n",
    "from geog0111.get_url import get_url\n",
    "\n",
    "doy,year = 273,2018\n",
    "tiles = ['h17v03', 'h18v03']\n",
    "\n",
    "url = get_url(doy,year).url\n",
    "html = nasa_requests.get(url).text\n",
    "soup = BeautifulSoup(html,'lxml')\n",
    "links = [mylink.attrs['href'] for mylink in soup.find_all('a')]\n",
    "\n",
    "tile_filenames = [item for item in links \\\n",
    "                      if (item.split('.')[-1] == 'hdf') and \\\n",
    "                         (item.split('.')[-4] in tiles)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise E3.2.4**\n",
    "\n",
    "* print out the first 10 items in `tile_filenames` and check the result is as you expect.\n",
    "* write a function called `modis_tiles()` that takes as input `doy`, `year` and `tiles` and returns a list of the modis tile **urls**.\n",
    "\n",
    "**Hint** \n",
    "\n",
    "1. Don't forget to put in a mechanism to allow you to change the default `base_url`, `product` and `version` (as you did for the function `get_url()`)\n",
    "\n",
    "2. In some circumstances, yopu can get repeats of filenames in the list. One way to get around this is to convert the list to a `numpy` array, and use [`np.unique()`](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.unique.html) to remove duplicates.\n",
    "\n",
    "        import numpy as np\n",
    "        tile_filenames = np.unique(tile_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do exercise here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should end up with something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geog0111.modis_tiles import modis_tiles\n",
    "\n",
    "doy,year = 273,2018\n",
    "tiles = ['h17v03', 'h18v03']\n",
    "\n",
    "tile_urls = modis_tiles(doy,year,tiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise E3.2.5**\n",
    "\n",
    "* print out the first 10 items in `tile_urls` and check the result is as you expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do exercise here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.4 Saving binary data to a file\n",
    "\n",
    "We suppose that we want to save the dataset to a local file on the system.\n",
    "\n",
    "It makes sense here to give the file the full MODIS filename. We need only then specify a directory ('folder') that we want to put the dataset in.\n",
    "\n",
    "We set this to be `data` here. Before we go any further we should check:\n",
    "\n",
    "* that the directory exists (if not, create it)\n",
    "* that the file we want to download doesn't already exist (else, don't bother)\n",
    "\n",
    "We can conveniently use methods in [`pathlib.Path`](https://docs.python.org/3/library/pathlib.html) for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018.09.30\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'destination_folder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-1cb06c2fe7fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# make sure destination_folder exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdest_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestination_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdest_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdest_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'destination_folder' is not defined"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "filename = url.split('/')[-1]\n",
    "print(filename)\n",
    "\n",
    "# make sure destination_folder exists\n",
    "dest_path = Path(destination_folder)\n",
    "if not dest_path.exists():\n",
    "    dest_path.mkdir()\n",
    "\n",
    "    # make a compound file name from folder and filename\n",
    "    output_fname = dest_path.joinpath(filename)\n",
    "\n",
    "    # does the file already exist?\n",
    "    if not output_fname.exists():\n",
    "        # save the data to file\n",
    "        with open(output_fname, 'wb') as fp:\n",
    "            w = fp.write(r.content)\n",
    "else:\n",
    "    print(f'{output_fname} exists')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.4 downloading the data file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function uses the `requests` library to pull the data from the URL and save it to a local file.\n",
    "\n",
    "You don't need to be able to write code of this complexity at the moment, though you might find it interesting to look at and work out what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geog0111.nasa_requests as nasa_requests\n",
    "from geog0111.modis_tiles import modis_tiles\n",
    "from pathlib import Path\n",
    "\n",
    "doy,year = 273,2018\n",
    "tiles = ['h17v03', 'h18v03']\n",
    "destination_folder = 'data'\n",
    "\n",
    "tile_urls = modis_tiles(doy,year,tiles)\n",
    "print(tile_urls)\n",
    "\n",
    "# loop over urls\n",
    "for url in tile_urls:\n",
    "    r = nasa_requests.get(url)\n",
    "    \n",
    "    # check response\n",
    "    if r.ok:\n",
    "        # get the filename from the url\n",
    "        filename = url.split('/')[-1]\n",
    "        print(filename)\n",
    "\n",
    "        # make sure destination_folder exists\n",
    "        dest_path = Path(destination_folder)\n",
    "        if not dest_path.exists():\n",
    "            dest_path.mkdir()\n",
    "\n",
    "            # make a compound file name from folder and filename\n",
    "            output_fname = dest_path.joinpath(filename)\n",
    "\n",
    "            # does the file already exist?\n",
    "            if not output_fname.exists():\n",
    "                # save the data to file\n",
    "                with open(output_fname, 'wb') as fp:\n",
    "                    w = fp.write(r.content)\n",
    "        else:\n",
    "            print(f'{output_fname} exists')\n",
    "    else:\n",
    "        print (f'response from {url} not good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise E3.2.6**\n",
    "\n",
    "* use the code above to write a function `get_modis_files()` that takes as input `doy`, `year` and `tiles`, has a default `destination_folder` of `data`, that downloads the appropriate datasets (if they don't already exist). It should have similar defaults to `modis_tiles()`. It should return a list of the output filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do exercise here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should end up with something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geog0111.get_modis_files import get_modis_files\n",
    "\n",
    "tiles = ['h17v03', 'h18v03']\n",
    "doy,year = today()\n",
    "doy -= 10\n",
    "\n",
    "# demonstrate use of **kwargs\n",
    "kwargs = {'base_url':'https://e4ftl01.cr.usgs.gov/MOTA',\n",
    "          'version':6,\n",
    "          'product':'MCD15A3H'}\n",
    "\n",
    "filenames = get_modis_files(doy,year,tiles,verbose=True,**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.5 Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will learn more fully how to visualise these later, but just to show that the datasets exist.\n",
    "\n",
    "You might want to look at the [FIPS](https://en.wikipedia.org/wiki/List_of_FIPS_country_codes) country codes for selecting boundary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import shutil \n",
    "'''\n",
    "Get the world borders shapefile that we will need\n",
    "'''\n",
    "tm_borders_url = \"http://thematicmapping.org/downloads/TM_WORLD_BORDERS-0.3.zip\"\n",
    "\n",
    "r = requests.get(tm_borders_url)\n",
    "with open(\"data/TM_WORLD_BORDERS-0.3.zip\", 'wb') as fp:\n",
    "    fp.write (r.content)\n",
    "\n",
    "shutil.unpack_archive(\"data/TM_WORLD_BORDERS-0.3.zip\",\n",
    "                     extract_dir=\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geog0111.get_modis_files import get_modis_files\n",
    "from geog0111.today import today\n",
    "import gdal\n",
    "import matplotlib.pylab as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import numpy as np\n",
    "\n",
    "def mosaic_and_mask_data(gdal_fnames, vector_file, vector_where):\n",
    "    stitch_vrt = gdal.BuildVRT(\"\", gdal_fnames)\n",
    "    g = gdal.Warp(\"\", stitch_vrt,\n",
    "                 format = 'MEM', dstNodata=200,\n",
    "                  cutlineDSName = vector_file,\n",
    "                  cutlineWhere = vector_where)\n",
    "    return g\n",
    "\n",
    "tiles = ['h17v03', 'h18v03']\n",
    "\n",
    "doy,year = today()\n",
    "doy -= 10\n",
    "kwargs = {'base_url':'https://e4ftl01.cr.usgs.gov/MOTA',\n",
    "          'version':6,\n",
    "          'product':'MCD15A3H'}\n",
    "\n",
    "filenames = get_modis_files(doy,year,tiles,**kwargs)\n",
    "\n",
    "# this part is to access a particular dataset in the file\n",
    "gdal_fnames = [f'HDF4_EOS:EOS_GRID:\"{file_name:s}\":MOD_Grid_MCD15A3H:Lai_500m'\n",
    "               for file_name in filenames]\n",
    "\n",
    "\n",
    "g = mosaic_and_mask_data(gdal_fnames, \"data/TM_WORLD_BORDERS-0.3.shp\",\n",
    "                         \"FIPS='UK'\")\n",
    "\n",
    "lai = np.array(g.ReadAsArray()).astype(float) * 0.1 # for LAI scaling\n",
    "# vcalid data mask\n",
    "mask = np.nonzero(lai < 20)\n",
    "min_y = mask[0].min()\n",
    "max_y = mask[0].max() + 1\n",
    "\n",
    "min_x = mask[1].min()\n",
    "max_x = mask[1].max() + 1\n",
    "\n",
    "lai = lai[min_y:max_y,\n",
    "               min_x:max_x]\n",
    "\n",
    "X0,XS,Xo,Y0,Yo,YS = g.GetGeoTransform()\n",
    "bounds = (X0,X0+XS*lai.shape[1],Y0,Y0+YS*lai.shape[0])\n",
    "\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "axs = plt.subplot(1,1,1,projection=ccrs.Sinusoidal())\n",
    "axs.set_extent(bounds,ccrs.Sinusoidal())\n",
    "im = axs.imshow(lai, interpolation=\"nearest\", vmin=0, vmax=6,\n",
    "             cmap=plt.cm.inferno_r, transform=ccrs.Sinusoidal(),extent=bounds)\n",
    "axs.set_title('LAI'+' '+str(tiles)+' '+str((doy,year)))\n",
    "fig.colorbar(im, ax=axs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geog0111.get_modis_files import get_modis_files\n",
    "from geog0111.today import today\n",
    "import gdal\n",
    "import matplotlib.pylab as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import numpy as np\n",
    "\n",
    "def mosaic_and_mask_data(gdal_fnames, vector_file, vector_where):\n",
    "    stitch_vrt = gdal.BuildVRT(\"\", gdal_fnames)\n",
    "    g = gdal.Warp(\"\", stitch_vrt,\n",
    "                 format = 'MEM', dstNodata=200,\n",
    "                  cutlineDSName = vector_file,\n",
    "                  cutlineWhere = vector_where)\n",
    "    return g\n",
    "\n",
    "tiles = ['h17v03', 'h18v03']\n",
    "\n",
    "doy,year = today()\n",
    "doy -= 10\n",
    "kwargs = {'base_url':'https://e4ftl01.cr.usgs.gov/MOTA',\n",
    "          'version':6,\n",
    "          'product':'MCD15A3H'}\n",
    "\n",
    "filenames = get_modis_files(doy,year,tiles,**kwargs)\n",
    "\n",
    "# this part is to access a particular dataset in the file\n",
    "gdal_fnames = [f'HDF4_EOS:EOS_GRID:\"{file_name:s}\":MOD_Grid_MCD15A3H:Lai_500m'\n",
    "               for file_name in filenames]\n",
    "\n",
    "g = mosaic_and_mask_data(gdal_fnames, \"data/TM_WORLD_BORDERS-0.3.shp\",\n",
    "                         \"FIPS='NL'\")\n",
    "\n",
    "lai = np.array(g.ReadAsArray()).astype(float) * 0.1 # for LAI scaling\n",
    "# vcalid data mask\n",
    "mask = np.nonzero(lai < 20)\n",
    "min_y = mask[0].min()\n",
    "max_y = mask[0].max() + 1\n",
    "\n",
    "min_x = mask[1].min()\n",
    "max_x = mask[1].max() + 1\n",
    "\n",
    "lai = lai[min_y:max_y,\n",
    "               min_x:max_x]\n",
    "\n",
    "X0,XS,Xo,Y0,Yo,YS = g.GetGeoTransform()\n",
    "bounds = (X0,X0+XS*lai.shape[1],Y0,Y0+YS*lai.shape[0])\n",
    "\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "axs = plt.subplot(1,1,1,projection=ccrs.Sinusoidal())\n",
    "axs.set_extent(bounds,ccrs.Sinusoidal())\n",
    "im = axs.imshow(lai, interpolation=\"nearest\", vmin=0, vmax=6,\n",
    "             cmap=plt.cm.inferno_r, transform=ccrs.Sinusoidal(),extent=bounds)\n",
    "axs.set_title('LAI'+' '+str(tiles)+' '+str((doy,year)))\n",
    "fig.colorbar(im, ax=axs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.2.7 Homework**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Have a look at the information for [`MOD10A1` product](http://www.icess.ucsb.edu/modis/SnowUsrGuide/usrguide_1dtil.html), which is the 500 m MODIS daily snow cover product.\n",
    "* Use what you have learned here to download the MOD10A product over the UK\n",
    "\n",
    "**Hint**: \n",
    "* The data are on a different server `https://n5eil01u.ecs.nsidc.org/MOST` \n",
    "* the template for the snow cover dataxset is `f'HDF4_EOS:EOS_GRID:\"{file_name:s}\":MOD_Grid_Snow_500m:NDSI_Snow_Cover'`\n",
    "* today-10 may not be the best example doy: choose something in winter\n",
    "* valid snow cover values are 0 to 100 (use this to set `vmin=0, vmax=100` when plotting)\n",
    "\n",
    "**N.B. You will be required to download this dataset for your assessed practical, so it is a good idea to sort code for this now**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do exercise here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.6 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session, we have learned how to download MODIS datasets from NASA Earthdata. \n",
    "\n",
    "We have developed and tested functions that group together the commands we want, ultimately arriving at the function `get_modis_files(doy,year,tiles,**kwargs)`.\n",
    "\n",
    "We have seen ((if you've done the homework) that such code is re-useable and can directly be used for your assessed practical."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "264.796875px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
